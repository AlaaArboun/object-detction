{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ultralytics\n",
    "!python -m pip install --extra-index-url https://google-coral.github.io/py-repo/ pycoral\n",
    "!pip install tensorflow==2.13.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\object-detection-coral\\myenv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from PIL import Image, ImageDraw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.15 üöÄ Python-3.11.4 torch-2.2.0+cpu CPU (Intel Core(TM) i7-1065G7 1.30GHz)\n",
      "YOLOv8m summary (fused): 218 layers, 25886080 parameters, 0 gradients, 78.9 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolov8m.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (49.7 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m starting export with tensorflow 2.15.0...\n",
      "WARNING ‚ö†Ô∏è tensorflow<=2.13.1 is required, but tensorflow==2.15.0 is currently installed https://github.com/ultralytics/ultralytics/issues/5161\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.15.0 opset 17...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m simplifying with onnxsim 0.4.35...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success ‚úÖ 13.0s, saved as 'yolov8m.onnx' (99.0 MB)\n",
      "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m running 'onnx2tf -i \"yolov8m.onnx\" -o \"yolov8m_saved_model\" -nuo --non_verbose'\n",
      "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m export success ‚úÖ 75.9s, saved as 'yolov8m_saved_model' (247.7 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTensorFlow Lite:\u001b[0m starting export with tensorflow 2.15.0...\n",
      "\u001b[34m\u001b[1mTensorFlow Lite:\u001b[0m export success ‚úÖ 0.0s, saved as 'yolov8m_saved_model\\yolov8m_float32.tflite' (99.0 MB)\n",
      "\n",
      "Export complete (80.2s)\n",
      "Results saved to \u001b[1mC:\\object-detection-coral\u001b[0m\n",
      "Predict:         yolo predict task=detect model=yolov8m_saved_model\\yolov8m_float32.tflite imgsz=640  \n",
      "Validate:        yolo val task=detect model=yolov8m_saved_model\\yolov8m_float32.tflite imgsz=640 data=coco.yaml  \n",
      "Visualize:       https://netron.app\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "model = YOLO('yolov8m.pt')\n",
    "results = model.export(format='tflite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\object-detection-coral\\myenv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Expected Input Dimensions: [640 640   3]\n",
      "Input Details:\n",
      "[{'name': 'inputs_0', 'index': 0, 'shape': array([  1, 640, 640,   3]), 'shape_signature': array([  1, 640, 640,   3]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "Output Details:\n",
      "[{'name': 'Identity', 'index': 514, 'shape': array([   1,   84, 8400]), 'shape_signature': array([   1,   84, 8400]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "(84, 8400)\n",
      "Image enregistr√©e avec succ√®s : output_images\\dog-cat.png\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageTk\n",
    "from tkinter import filedialog\n",
    "import tensorflow as tf\n",
    "import tflite_runtime.interpreter as tflite\n",
    "from tflite_runtime.interpreter import load_delegate\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Load the TensorFlow Lite model with Edge TPU delegate\n",
    "model_path = 'yolov8m_saved_model/yolov8m_float32.tflite'\n",
    "interpreter = tflite.Interpreter(model_path=model_path, experimental_delegates=[load_delegate('edgetpu.dll')])\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "input_shape = input_details[0]['shape'][1:]\n",
    "print(\"Expected Input Dimensions:\", input_details[0]['shape'][1:])\n",
    "\n",
    "# Print input and output details\n",
    "print(\"Input Details:\")\n",
    "print(input_details)\n",
    "print(\"Output Details:\")\n",
    "print(output_details)\n",
    "\n",
    "# Threshold Setting\n",
    "threshold = 0.03\n",
    "\n",
    "# Load the class labels from the JSON file\n",
    "with open('C:\\object-detection-coral\\label_files\\labels_coco.json', 'r') as json_file:\n",
    "    class_labels = json.load(json_file)\n",
    "\n",
    "# Create a tkinter window\n",
    "root = tk.Tk()\n",
    "root.title(\"Object Detection\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# save_image_with_boxes\n",
    "output_directory = \"output_images\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "def save_image_with_boxes(image, file_path):\n",
    "    output_path = os.path.join(output_directory, os.path.basename(file_path))\n",
    "    cv2.imwrite(output_path, image)\n",
    "    print(f\"Image enregistr√©e avec succ√®s : {output_path}\")\n",
    "\n",
    "\n",
    "\n",
    "# Preprocessing function using TensorFlow\n",
    "def preprocess(image_path, input_shape):\n",
    "    # Read image file\n",
    "    raw = tf.io.read_file(image_path)\n",
    "    # Decode image\n",
    "    image = tf.image.decode_image(raw, channels=3)\n",
    "    # Resize image\n",
    "    image_resized = tf.image.resize(image, (input_shape[0], input_shape[1]))\n",
    "    # Convert image to float32\n",
    "    image_float32 = tf.cast(image_resized, tf.float32)\n",
    "    # Normalize image\n",
    "    image_normalized = image_float32 / 255.0\n",
    "    # Add batch dimension\n",
    "    image_with_batch = tf.expand_dims(image_normalized, 0)\n",
    "\n",
    "    return image_with_batch\n",
    "\n",
    "# Function to open a file dialog and select an image\n",
    "def open_image():\n",
    "    file_path = filedialog.askopenfilename()\n",
    "    if file_path:\n",
    "        # Preprocess the image\n",
    "        image_np = preprocess(file_path, input_shape)\n",
    "\n",
    "        # Perform inference\n",
    "        interpreter.set_tensor(input_details[0]['index'], image_np)\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details[0]['index'])\n",
    "        output = output[0]\n",
    "        print(output.shape)\n",
    "\n",
    "        # Read the image with OpenCV to draw bounding boxes\n",
    "        image = cv2.imread(file_path)\n",
    "        orig_h, orig_w = image.shape[:2]\n",
    "\n",
    "        for i in range(output.shape[1]):\n",
    "            detection = output[:, i]\n",
    "            x_center, y_center, width, height = detection[:4]\n",
    "            x1 = max(0, int((x_center - width / 2) * orig_w))\n",
    "            y1 = max(0, int((y_center - height / 2) * orig_h))\n",
    "            x2 = min(orig_w - 1, int((x_center + width / 2) * orig_w))\n",
    "            y2 = min(orig_h - 1, int((y_center + height / 2) * orig_h))\n",
    "            score = np.max(detection[4:])\n",
    "            cls = np.argmax(detection[4:])\n",
    "            if score >= threshold:\n",
    "                class_name = class_labels[str(cls)]  # Obtain the class name using the class index\n",
    "                cv2.rectangle(image, (x1, y1), (x2, y2), (0, 0, 255), thickness=2)  # Draw bounding box\n",
    "                cv2.putText(image, f\"{class_name}: {score:.2f}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "        # Convert the image to RGB format for display with Tkinter\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image_pil = Image.fromarray(image_rgb)\n",
    "        tk_image = ImageTk.PhotoImage(image_pil)\n",
    "\n",
    "        # Update the image label with the new image\n",
    "        image_label.config(image=tk_image)\n",
    "        image_label.image = tk_image\n",
    "        save_image_with_boxes(image, file_path)\n",
    "\n",
    "# Button to select an image\n",
    "select_button = tk.Button(root, text=\"Select Image\", command=open_image)\n",
    "select_button.pack()\n",
    "\n",
    "# Create a label to initially display the image\n",
    "image_label = tk.Label(root)\n",
    "image_label.pack()\n",
    "\n",
    "# Run the tkinter event loop\n",
    "root.mainloop()\n",
    "#s7i7a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
