{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Input Dimensions: [640 640   3]\n",
      "Input Details:\n",
      "[{'name': 'inputs_0', 'index': 0, 'shape': array([  1, 640, 640,   3]), 'shape_signature': array([  1, 640, 640,   3]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "Output Details:\n",
      "[{'name': 'Identity', 'index': 516, 'shape': array([   1,   84, 8400]), 'shape_signature': array([   1,   84, 8400]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "(84, 8400)\n",
      "Image enregistrée avec succès : output_images\\dog-cat.png\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageTk\n",
    "from tkinter import filedialog\n",
    "import json\n",
    "import os\n",
    "from tflite_runtime.interpreter import Interpreter, load_delegate\n",
    "\n",
    "# Load the TensorFlow Lite model with Edge TPU delegate\n",
    "model_path = 'yolov8m_saved_model/yolov8m_float32.tflite'\n",
    "interpreter = Interpreter(model_path=model_path, experimental_delegates=[load_delegate('edgetpu.dll')])\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "input_shape = input_details[0]['shape'][1:]\n",
    "print(\"Expected Input Dimensions:\", input_details[0]['shape'][1:])\n",
    "\n",
    "# Print input and output details\n",
    "print(\"Input Details:\")\n",
    "print(input_details)\n",
    "print(\"Output Details:\")\n",
    "print(output_details)\n",
    "\n",
    "# Threshold Setting\n",
    "threshold = 0.3\n",
    "\n",
    "# Load the class labels from the JSON file\n",
    "with open('C:\\object-detection-coral\\label_files\\labels_coco.json', 'r') as json_file:\n",
    "    class_labels = json.load(json_file)\n",
    "\n",
    "# Create a tkinter window\n",
    "root = tk.Tk()\n",
    "root.title(\"Object Detection\")\n",
    "\n",
    "# save_image_with_boxes\n",
    "output_directory = \"output_images\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "def save_image_with_boxes(image, file_path):\n",
    "    output_path = os.path.join(output_directory, os.path.basename(file_path))\n",
    "    cv2.imwrite(output_path, image)\n",
    "    print(f\"Image enregistrée avec succès : {output_path}\")\n",
    "\n",
    "# Preprocessing function using OpenCV (cv2)\n",
    "def preprocess(image_path, input_shape):\n",
    "    # Read image file\n",
    "    image = cv2.imread(image_path)\n",
    "    # Resize image\n",
    "    image_resized = cv2.resize(image, (input_shape[0], input_shape[1]))\n",
    "    # Convert image to float32\n",
    "    image_float32 = image_resized.astype(np.float32)\n",
    "    # Normalize image\n",
    "    image_normalized = image_float32 / 255.0\n",
    "    # Add batch dimension\n",
    "    image_with_batch = np.expand_dims(image_normalized, 0)\n",
    "\n",
    "    return image_with_batch\n",
    "\n",
    "# Function to open a file dialog and select an image\n",
    "def open_image():\n",
    "    file_path = filedialog.askopenfilename()\n",
    "    if file_path:\n",
    "        # Preprocess the image\n",
    "        image_np = preprocess(file_path, input_shape)\n",
    "\n",
    "        # Perform inference\n",
    "        interpreter.set_tensor(input_details[0]['index'], image_np)\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details[0]['index'])\n",
    "        output = output[0]\n",
    "        print(output.shape)\n",
    "\n",
    "        # Read the image with OpenCV to draw bounding boxes\n",
    "        image = cv2.imread(file_path)\n",
    "        orig_h, orig_w = image.shape[:2]\n",
    "\n",
    "        for i in range(output.shape[1]):\n",
    "            detection = output[:, i]\n",
    "            x_center, y_center, width, height = detection[:4]\n",
    "            x1 = max(0, int((x_center - width / 2) * orig_w))\n",
    "            y1 = max(0, int((y_center - height / 2) * orig_h))\n",
    "            x2 = min(orig_w - 1, int((x_center + width / 2) * orig_w))\n",
    "            y2 = min(orig_h - 1, int((y_center + height / 2) * orig_h))\n",
    "           \n",
    "            score = np.max(detection[4:])\n",
    "         \n",
    "            cls = np.argmax(detection[4:])\n",
    "            if score >= threshold:\n",
    "                class_name = class_labels[str(cls)]  # Obtain the class name using the class index\n",
    "                cv2.rectangle(image, (x1, y1), (x2, y2), (0, 0, 255), thickness=2)  # Draw bounding box\n",
    "                cv2.putText(image, f\"{class_name}: {score:.2f}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "        # Convert the image to RGB format for display with Tkinter\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image_pil = Image.fromarray(image_rgb)\n",
    "        tk_image = ImageTk.PhotoImage(image_pil)\n",
    "\n",
    "        # Update the image label with the new image\n",
    "        image_label.config(image=tk_image)\n",
    "        image_label.image = tk_image\n",
    "        save_image_with_boxes(image, file_path)\n",
    "\n",
    "# Button to select an image\n",
    "select_button = tk.Button(root, text=\"Select Image\", command=open_image)\n",
    "select_button.pack()\n",
    "\n",
    "# Create a label to initially display the image\n",
    "image_label = tk.Label(root)\n",
    "image_label.pack()\n",
    "\n",
    "# Run the tkinter event loop\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Input Dimensions: [640 640   3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python311\\Lib\\tkinter\\__init__.py\", line 1948, in __call__\n",
      "    return self.func(*args)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ala Arboun\\AppData\\Local\\Temp\\ipykernel_7584\\2163013572.py\", line 85, in open_video\n",
      "    process_video(file_path)\n",
      "  File \"C:\\Users\\Ala Arboun\\AppData\\Local\\Temp\\ipykernel_7584\\2163013572.py\", line 48, in process_video\n",
      "    image_np = preprocess(frame, input_shape)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ala Arboun\\AppData\\Local\\Temp\\ipykernel_7584\\3932583319.py\", line 48, in preprocess\n",
      "    image = cv2.imread(image_path)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: Can't convert object to 'str' for 'filename'\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python311\\Lib\\tkinter\\__init__.py\", line 1948, in __call__\n",
      "    return self.func(*args)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ala Arboun\\AppData\\Local\\Temp\\ipykernel_7584\\2163013572.py\", line 85, in open_video\n",
      "    process_video(file_path)\n",
      "  File \"C:\\Users\\Ala Arboun\\AppData\\Local\\Temp\\ipykernel_7584\\2163013572.py\", line 48, in process_video\n",
      "    image_np = preprocess(frame, input_shape)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ala Arboun\\AppData\\Local\\Temp\\ipykernel_7584\\3932583319.py\", line 48, in preprocess\n",
      "    image = cv2.imread(image_path)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: Can't convert object to 'str' for 'filename'\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageTk\n",
    "from tkinter import filedialog\n",
    "import json\n",
    "import os\n",
    "from tflite_runtime.interpreter import Interpreter, load_delegate\n",
    "\n",
    "# Load the TensorFlow Lite model with Edge TPU delegate\n",
    "model_path = 'yolov8m_saved_model/yolov8m_float32.tflite'\n",
    "interpreter = Interpreter(model_path=model_path, experimental_delegates=[load_delegate('edgetpu.dll')])\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "input_shape = input_details[0]['shape'][1:]\n",
    "print(\"Expected Input Dimensions:\", input_details[0]['shape'][1:])\n",
    "\n",
    "# Threshold Setting\n",
    "threshold = 0.3\n",
    "\n",
    "# Load the class labels from the JSON file\n",
    "with open('C:\\object-detection-coral\\label_files\\labels_coco.json', 'r') as json_file:\n",
    "    class_labels = json.load(json_file)\n",
    "\n",
    "# Create a tkinter window\n",
    "root = tk.Tk()\n",
    "root.title(\"Object Detection\")\n",
    "\n",
    "# save_image_with_boxes\n",
    "output_directory = \"output_images\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "def save_image_with_boxes(image, file_path):\n",
    "    output_path = os.path.join(output_directory, os.path.basename(file_path))\n",
    "    cv2.imwrite(output_path, image)\n",
    "    print(f\"Image saved successfully: {output_path}\")\n",
    "\n",
    "# Function to process video for object detection\n",
    "def process_video(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Preprocess the frame\n",
    "        image_np = preprocess(frame, input_shape)\n",
    "\n",
    "        # Perform inference\n",
    "        interpreter.set_tensor(input_details[0]['index'], image_np)\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details[0]['index'])\n",
    "        output = output[0]\n",
    "\n",
    "        # Draw bounding boxes\n",
    "        orig_h, orig_w = frame.shape[:2]\n",
    "        for i in range(output.shape[1]):\n",
    "            detection = output[:, i]\n",
    "            x_center, y_center, width, height = detection[:4]\n",
    "            x1 = max(0, int((x_center - width / 2) * orig_w))\n",
    "            y1 = max(0, int((y_center - height / 2) * orig_h))\n",
    "            x2 = min(orig_w - 1, int((x_center + width / 2) * orig_w))\n",
    "            y2 = min(orig_h - 1, int((y_center + height / 2) * orig_h))\n",
    "            \n",
    "            score = np.max(detection[4:])\n",
    "            cls = np.argmax(detection[4:])\n",
    "            if score >= threshold:\n",
    "                class_name = class_labels[str(cls)]  # Obtain the class name using the class index\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), thickness=2)  # Draw bounding box\n",
    "                cv2.putText(frame, f\"{class_name}: {score:.2f}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "        # Display the processed frame\n",
    "        cv2.imshow('Object Detection', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Function to open a file dialog and select a video\n",
    "def open_video():\n",
    "    file_path = filedialog.askopenfilename()\n",
    "    if file_path:\n",
    "        process_video(file_path)\n",
    "\n",
    "# Button to select a video\n",
    "select_button = tk.Button(root, text=\"Select Video\", command=open_video)\n",
    "select_button.pack()\n",
    "\n",
    "# Run the tkinter event loop\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Input Dimensions: [640 640   3]\n",
      "Average FPS: 0.7666737660657404\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageTk\n",
    "from tkinter import filedialog\n",
    "import json\n",
    "import os\n",
    "from tflite_runtime.interpreter import Interpreter, load_delegate\n",
    "\n",
    "# Load the TensorFlow Lite model with Edge TPU delegate\n",
    "model_path = 'yolov8m_saved_model/yolov8m_float32.tflite'\n",
    "interpreter = Interpreter(model_path=model_path, experimental_delegates=[load_delegate('edgetpu.dll')])\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "input_shape = input_details[0]['shape'][1:]\n",
    "print(\"Expected Input Dimensions:\", input_details[0]['shape'][1:])\n",
    "\n",
    "# Threshold Setting\n",
    "threshold = 0.3\n",
    "\n",
    "# Load the class labels from the JSON file\n",
    "with open('C:\\object-detection-coral\\label_files\\labels_coco.json', 'r') as json_file:\n",
    "    class_labels = json.load(json_file)\n",
    "\n",
    "# Create a tkinter window\n",
    "root = tk.Tk()\n",
    "root.title(\"Object Detection\")\n",
    "\n",
    "# save_image_with_boxes\n",
    "output_directory = \"output_images\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "def save_image_with_boxes(image, file_path):\n",
    "    output_path = os.path.join(output_directory, os.path.basename(file_path))\n",
    "    cv2.imwrite(output_path, image)\n",
    "    print(f\"Image saved successfully: {output_path}\")\n",
    "\n",
    "# Function to preprocess the frame\n",
    "def preprocess(frame, input_shape):\n",
    "    # Resize frame\n",
    "    image_resized = cv2.resize(frame, (input_shape[0], input_shape[1]))\n",
    "    # Convert image to float32\n",
    "    image_float32 = image_resized.astype(np.float32)\n",
    "    # Normalize image\n",
    "    image_normalized = image_float32 / 255.0\n",
    "    # Add batch dimension\n",
    "    image_with_batch = np.expand_dims(image_normalized, 0)\n",
    "\n",
    "    return image_with_batch\n",
    "\n",
    "# Function to process video for object detection\n",
    "# Function to process video for object detection\n",
    "def process_video(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    start_time = cv2.getTickCount() # Get initial time\n",
    "    frame_count = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Increment frame count\n",
    "        frame_count += 1\n",
    "\n",
    "        # Preprocess the frame\n",
    "        image_np = preprocess(frame, input_shape)\n",
    "\n",
    "        # Perform inference\n",
    "        interpreter.set_tensor(input_details[0]['index'], image_np)\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details[0]['index'])\n",
    "        output = output[0]\n",
    "\n",
    "        # Draw bounding boxes\n",
    "        orig_h, orig_w = frame.shape[:2]\n",
    "        for i in range(output.shape[1]):\n",
    "            detection = output[:, i]\n",
    "            x_center, y_center, width, height = detection[:4]\n",
    "            x1 = max(0, int((x_center - width / 2) * orig_w))\n",
    "            y1 = max(0, int((y_center - height / 2) * orig_h))\n",
    "            x2 = min(orig_w - 1, int((x_center + width / 2) * orig_w))\n",
    "            y2 = min(orig_h - 1, int((y_center + height / 2) * orig_h))\n",
    "            \n",
    "            score = np.max(detection[4:])\n",
    "            cls = np.argmax(detection[4:])\n",
    "            if score >= threshold:\n",
    "                class_name = class_labels[str(cls)]  # Obtain the class name using the class index\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), thickness=2)  # Draw bounding box\n",
    "                cv2.putText(frame, f\"{class_name}: {score:.2f}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "        # Display the processed frame\n",
    "        cv2.imshow('Object Detection', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Calculate FPS\n",
    "    end_time = cv2.getTickCount()\n",
    "    total_time = (end_time - start_time) / cv2.getTickFrequency() # in seconds\n",
    "    fps = frame_count / total_time\n",
    "\n",
    "    print(f\"Average FPS: {fps}\")\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Function to open a file dialog and select a video\n",
    "def open_video():\n",
    "    file_path = filedialog.askopenfilename()\n",
    "    if file_path:\n",
    "        process_video(file_path)\n",
    "\n",
    "# Button to select a video\n",
    "select_button = tk.Button(root, text=\"Select Video\", command=open_video)\n",
    "select_button.pack()\n",
    "\n",
    "# Run the tkinter event loop\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Input Dimensions: [640 640   3]\n",
      "Average FPS: 0.2630490581890164\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageTk\n",
    "from tkinter import filedialog\n",
    "import json\n",
    "import os\n",
    "from tflite_runtime.interpreter import Interpreter, load_delegate\n",
    "\n",
    "# Load the TensorFlow Lite model with Edge TPU delegate\n",
    "model_path = 'yolov8m_saved_model\\yolov8m_integer_quant.tflite'\n",
    "interpreter = Interpreter(model_path=model_path, experimental_delegates=[load_delegate('edgetpu.dll')])\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "input_shape = input_details[0]['shape'][1:]\n",
    "print(\"Expected Input Dimensions:\", input_details[0]['shape'][1:])\n",
    "\n",
    "# Threshold Setting\n",
    "threshold = 0.3\n",
    "\n",
    "# Load the class labels from the JSON file\n",
    "with open('C:\\object-detection-coral\\label_files\\labels_coco.json', 'r') as json_file:\n",
    "    class_labels = json.load(json_file)\n",
    "\n",
    "# Create a tkinter window\n",
    "root = tk.Tk()\n",
    "root.title(\"Object Detection\")\n",
    "\n",
    "# save_image_with_boxes\n",
    "output_directory = \"output_images\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "def save_image_with_boxes(image, file_path):\n",
    "    output_path = os.path.join(output_directory, os.path.basename(file_path))\n",
    "    cv2.imwrite(output_path, image)\n",
    "    print(f\"Image saved successfully: {output_path}\")\n",
    "\n",
    "# Function to preprocess the frame\n",
    "def preprocess(frame, input_shape):\n",
    "    # Resize frame\n",
    "    image_resized = cv2.resize(frame, (input_shape[0], input_shape[1]))\n",
    "    # Convert image to float32\n",
    "    image_float32 = image_resized.astype(np.float32)\n",
    "    # Normalize image\n",
    "    image_normalized = image_float32 / 255.0\n",
    "    # Add batch dimension\n",
    "    image_with_batch = np.expand_dims(image_normalized, 0)\n",
    "\n",
    "    return image_with_batch\n",
    "\n",
    "# Function to process video for object detection\n",
    "# Function to process video for object detection\n",
    "def process_video(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    start_time = cv2.getTickCount() # Get initial time\n",
    "    frame_count = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Increment frame count\n",
    "        frame_count += 1\n",
    "\n",
    "        # Preprocess the frame\n",
    "        image_np = preprocess(frame, input_shape)\n",
    "\n",
    "        # Perform inference\n",
    "        interpreter.set_tensor(input_details[0]['index'], image_np)\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details[0]['index'])\n",
    "        output = output[0]\n",
    "\n",
    "        # Draw bounding boxes\n",
    "        orig_h, orig_w = frame.shape[:2]\n",
    "        for i in range(output.shape[1]):\n",
    "            detection = output[:, i]\n",
    "            x_center, y_center, width, height = detection[:4]\n",
    "            x1 = max(0, int((x_center - width / 2) * orig_w))\n",
    "            y1 = max(0, int((y_center - height / 2) * orig_h))\n",
    "            x2 = min(orig_w - 1, int((x_center + width / 2) * orig_w))\n",
    "            y2 = min(orig_h - 1, int((y_center + height / 2) * orig_h))\n",
    "            \n",
    "            score = np.max(detection[4:])\n",
    "            cls = np.argmax(detection[4:])\n",
    "            if score >= threshold:\n",
    "                class_name = class_labels[str(cls)]  # Obtain the class name using the class index\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), thickness=2)  # Draw bounding box\n",
    "                cv2.putText(frame, f\"{class_name}: {score:.2f}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "        # Display the processed frame\n",
    "        cv2.imshow('Object Detection', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Calculate FPS\n",
    "    end_time = cv2.getTickCount()\n",
    "    total_time = (end_time - start_time) / cv2.getTickFrequency() # in seconds\n",
    "    fps = frame_count / total_time\n",
    "\n",
    "    print(f\"Average FPS: {fps}\")\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Function to open a file dialog and select a video\n",
    "def open_video():\n",
    "    file_path = filedialog.askopenfilename()\n",
    "    if file_path:\n",
    "        process_video(file_path)\n",
    "\n",
    "# Button to select a video\n",
    "select_button = tk.Button(root, text=\"Select Video\", command=open_video)\n",
    "select_button.pack()\n",
    "\n",
    "# Run the tkinter event loop\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Input Dimensions: [640 640   3]\n",
      "Average FPS: 1.329856935320755\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageTk\n",
    "from tkinter import filedialog\n",
    "import json\n",
    "import os\n",
    "from tflite_runtime.interpreter import Interpreter, load_delegate\n",
    "\n",
    "# Load the TensorFlow Lite model with Edge TPU delegate\n",
    "model_path = 'yolov8m_saved_model/yolov8m_integer_quant.tflite'\n",
    "interpreter = Interpreter(model_path=model_path, experimental_delegates=[load_delegate('edgetpu.dll')])\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "input_shape = input_details[0]['shape'][1:]\n",
    "print(\"Expected Input Dimensions:\", input_details[0]['shape'][1:])\n",
    "\n",
    "# Threshold Setting\n",
    "threshold = 0.3\n",
    "\n",
    "# Load the class labels from the JSON file\n",
    "with open('C:/object-detection-coral/label_files/labels_coco.json', 'r') as json_file:\n",
    "    class_labels = json.load(json_file)\n",
    "\n",
    "# Create a tkinter window\n",
    "root = tk.Tk()\n",
    "root.title(\"Object Detection\")\n",
    "\n",
    "# save_image_with_boxes\n",
    "output_directory = \"output_images\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "def save_image_with_boxes(image, file_path):\n",
    "    output_path = os.path.join(output_directory, os.path.basename(file_path))\n",
    "    cv2.imwrite(output_path, image)\n",
    "    print(f\"Image saved successfully: {output_path}\")\n",
    "\n",
    "# Function to preprocess the frame\n",
    "def preprocess(frame, input_shape):\n",
    "    # Resize frame\n",
    "    image_resized = cv2.resize(frame, (input_shape[0], input_shape[1]))\n",
    "    # Convert image to float32\n",
    "    image_float32 = image_resized.astype(np.float32)\n",
    "    # Normalize image\n",
    "    image_normalized = image_float32 / 255.0\n",
    "    # Add batch dimension\n",
    "    image_with_batch = np.expand_dims(image_normalized, 0)\n",
    "\n",
    "    return image_with_batch\n",
    "\n",
    "# Function to process video for object detection\n",
    "def process_video(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    start_time = cv2.getTickCount() # Get initial time\n",
    "    frame_count = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Increment frame count\n",
    "        frame_count += 1\n",
    "\n",
    "        # Preprocess the frame\n",
    "        image_np = preprocess(frame, input_shape)\n",
    "\n",
    "        # Perform inference\n",
    "        interpreter.set_tensor(input_details[0]['index'], image_np)\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details[0]['index'])\n",
    "        output = output[0]\n",
    "\n",
    "        # Draw bounding boxes\n",
    "        orig_h, orig_w = frame.shape[:2]\n",
    "        for i in range(output.shape[1]):\n",
    "            detection = output[:, i]\n",
    "            x_center, y_center, width, height = detection[:4]\n",
    "            x1 = max(0, int((x_center - width / 2) * orig_w))\n",
    "            y1 = max(0, int((y_center - height / 2) * orig_h))\n",
    "            x2 = min(orig_w - 1, int((x_center + width / 2) * orig_w))\n",
    "            y2 = min(orig_h - 1, int((y_center + height / 2) * orig_h))\n",
    "            \n",
    "            score = np.max(detection[4:])\n",
    "            cls = np.argmax(detection[4:])\n",
    "            if score >= threshold:\n",
    "                class_name = class_labels[str(cls)]  # Obtain the class name using the class index\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), thickness=2)  # Draw bounding box\n",
    "                cv2.putText(frame, f\"{class_name}: {score:.2f}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "        # Display the processed frame\n",
    "        cv2.imshow('Object Detection', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Calculate FPS\n",
    "    end_time = cv2.getTickCount()\n",
    "    total_time = (end_time - start_time) / cv2.getTickFrequency() # in seconds\n",
    "    fps = frame_count / total_time\n",
    "\n",
    "    print(f\"Average FPS: {fps}\")\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "# Function to open a file dialog and select a video\n",
    "def open_video():\n",
    "    file_path = filedialog.askopenfilename()\n",
    "    if file_path:\n",
    "        process_video(file_path)\n",
    "\n",
    "# Button to select a video\n",
    "select_button = tk.Button(root, text=\"Select Video\", command=open_video)\n",
    "select_button.pack()\n",
    "\n",
    "# Run the tkinter event loop\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Input Dimensions: [224 224   3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-15 (inference_thread):\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python311\\Lib\\threading.py\", line 1038, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\object-detection-coral\\venv-inference\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 761, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"C:\\Python311\\Lib\\threading.py\", line 975, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\Ala Arboun\\AppData\\Local\\Temp\\ipykernel_13820\\324957765.py\", line 65, in inference_thread\n",
      "  File \"c:\\object-detection-coral\\venv-inference\\Lib\\site-packages\\tflite_runtime\\interpreter.py\", line 940, in invoke\n",
      "    self._ensure_safe()\n",
      "  File \"c:\\object-detection-coral\\venv-inference\\Lib\\site-packages\\tflite_runtime\\interpreter.py\", line 556, in _ensure_safe\n",
      "    raise RuntimeError(\"\"\"There is at least 1 reference to internal data\n",
      "RuntimeError: There is at least 1 reference to internal data\n",
      "      in the interpreter in the form of a numpy array or slice. Be sure to\n",
      "      only hold the function returned from tensor() if you are using raw\n",
      "      data access.\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tkinter import filedialog\n",
    "import json\n",
    "import os\n",
    "from tflite_runtime.interpreter import Interpreter, load_delegate\n",
    "import threading\n",
    "\n",
    "# Load the TensorFlow Lite model with Edge TPU delegate\n",
    "model_path = 'yolov8m_saved_model/yolov8m_integer_quant.tflite'\n",
    "interpreter = Interpreter(model_path=model_path, experimental_delegates=[load_delegate('edgetpu.dll')])\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "input_shape = input_details[0]['shape'][1:]\n",
    "print(\"Expected Input Dimensions:\", input_details[0]['shape'][1:])\n",
    "\n",
    "# Threshold Setting\n",
    "threshold = 0.3\n",
    "\n",
    "# Load the class labels from the JSON file\n",
    "with open('C:/object-detection-coral/label_files/labels_coco.json', 'r') as json_file:\n",
    "    class_labels = json.load(json_file)\n",
    "\n",
    "# Create a tkinter window\n",
    "root = tk.Tk()\n",
    "root.title(\"Object Detection\")\n",
    "\n",
    "# save_image_with_boxes\n",
    "output_directory = \"output_images\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Function to preprocess the frame\n",
    "def preprocess(frame, input_shape):\n",
    "    # Resize frame\n",
    "    image_resized = cv2.resize(frame, (input_shape[0], input_shape[1]))\n",
    "    # Convert image to float32\n",
    "    image_float32 = image_resized.astype(np.float32)\n",
    "    # Normalize image\n",
    "    image_normalized = image_float32 / 255.0\n",
    "    # Add batch dimension\n",
    "    image_with_batch = np.expand_dims(image_normalized, 0)\n",
    "\n",
    "    return image_with_batch\n",
    "\n",
    "# Function to perform inference on frames\n",
    "def inference_thread(cap):\n",
    "    frame_count = 0\n",
    "    start_time = cv2.getTickCount()  # Get initial time\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Increment frame count\n",
    "        frame_count += 1\n",
    "\n",
    "        # Preprocess the frame\n",
    "        image_np = preprocess(frame, input_shape)\n",
    "\n",
    "        # Perform inference\n",
    "        interpreter.set_tensor(input_details[0]['index'], image_np)\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details[0]['index'])\n",
    "        output = output[0]\n",
    "\n",
    "        # Draw bounding boxes\n",
    "        orig_h, orig_w = frame.shape[:2]\n",
    "        for i in range(output.shape[1]):\n",
    "            detection = output[:, i]\n",
    "            x_center, y_center, width, height = detection[:4]\n",
    "            x1 = max(0, int((x_center - width / 2) * orig_w))\n",
    "            y1 = max(0, int((y_center - height / 2) * orig_h))\n",
    "            x2 = min(orig_w - 1, int((x_center + width / 2) * orig_w))\n",
    "            y2 = min(orig_h - 1, int((y_center + height / 2) * orig_h))\n",
    "            \n",
    "            score = np.max(detection[4:])\n",
    "            cls = np.argmax(detection[4:])\n",
    "            if score >= threshold:\n",
    "                class_name = class_labels[str(cls)]  # Obtain the class name using the class index\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), thickness=2)  # Draw bounding box\n",
    "                cv2.putText(frame, f\"{class_name}: {score:.2f}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "        # Display the processed frame\n",
    "        cv2.imshow('Object Detection', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Calculate FPS\n",
    "    end_time = cv2.getTickCount()\n",
    "    total_time = (end_time - start_time) / cv2.getTickFrequency() # in seconds\n",
    "    fps = frame_count / total_time\n",
    "\n",
    "    print(f\"Average FPS: {fps}\")\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Function to open a file dialog and select a video\n",
    "def open_video():\n",
    "    file_path = filedialog.askopenfilename()\n",
    "    if file_path:\n",
    "        cap = cv2.VideoCapture(file_path)\n",
    "        # Start a new thread for inference\n",
    "        threading.Thread(target=inference_thread, args=(cap,), daemon=True).start()\n",
    "\n",
    "# Button to select a video\n",
    "select_button = tk.Button(root, text=\"Select Video\", command=open_video)\n",
    "select_button.pack()\n",
    "\n",
    "# Run the tkinter event loop\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Input Dimensions: [224 224   3]\n",
      "Average FPS: 0.9303917964137249\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tkinter import filedialog\n",
    "import json\n",
    "import os\n",
    "from tflite_runtime.interpreter import Interpreter, load_delegate\n",
    "import threading\n",
    "from queue import Queue\n",
    "\n",
    "# Load the TensorFlow Lite model with Edge TPU delegate\n",
    "model_path = 'yolov8m_saved_model/yolov8m_integer_quant.tflite'\n",
    "interpreter = Interpreter(model_path=model_path, experimental_delegates=[load_delegate('edgetpu.dll')])\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "input_shape = input_details[0]['shape'][1:]\n",
    "print(\"Expected Input Dimensions:\", input_details[0]['shape'][1:])\n",
    "\n",
    "# Threshold Setting\n",
    "threshold = 0.3\n",
    "\n",
    "# Load the class labels from the JSON file\n",
    "with open('C:/object-detection-coral/label_files/labels_coco.json', 'r') as json_file:\n",
    "    class_labels = json.load(json_file)\n",
    "\n",
    "# Create a tkinter window\n",
    "root = tk.Tk()\n",
    "root.title(\"Object Detection\")\n",
    "\n",
    "# save_image_with_boxes\n",
    "output_directory = \"output_images\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Queue for preprocessed frames\n",
    "preprocessed_queue = Queue(maxsize=5)  # Adjust the size according to your needs\n",
    "\n",
    "# Lock for synchronization\n",
    "lock = threading.Lock()\n",
    "\n",
    "# Function to preprocess the frame\n",
    "def preprocess_frame(frame, input_shape):\n",
    "    # Resize frame\n",
    "    image_resized = cv2.resize(frame, (input_shape[0], input_shape[1]))\n",
    "    # Convert image to float32\n",
    "    image_float32 = image_resized.astype(np.float32)\n",
    "    # Normalize image\n",
    "    image_normalized = image_float32 / 255.0\n",
    "    # Add batch dimension\n",
    "    image_with_batch = np.expand_dims(image_normalized, 0)\n",
    "    return image_with_batch\n",
    "\n",
    "# Preprocessing thread function\n",
    "def preprocessing_thread(cap):\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # Preprocess the frame\n",
    "        preprocessed_frame = preprocess_frame(frame, input_shape)\n",
    "        # Get original frame dimensions\n",
    "        orig_h, orig_w = frame.shape[:2]\n",
    "        # Put the preprocessed frame and its dimensions into the queue\n",
    "        preprocessed_queue.put((preprocessed_frame, frame, orig_h, orig_w))\n",
    "    cap.release()\n",
    "\n",
    "# Function to perform inference on frames\n",
    "def inference_thread():\n",
    "    frame_count = 0\n",
    "    start_time = cv2.getTickCount()  # Get initial time\n",
    "    while True:\n",
    "        if not preprocessed_queue.empty():\n",
    "            # Get a preprocessed frame from the queue\n",
    "            preprocessed_frame, frame, orig_h, orig_w = preprocessed_queue.get()\n",
    "            frame_count += 1\n",
    "\n",
    "            # Perform inference\n",
    "            interpreter.set_tensor(input_details[0]['index'], preprocessed_frame)\n",
    "            interpreter.invoke()\n",
    "            output = interpreter.get_tensor(output_details[0]['index'])\n",
    "            output = output[0]\n",
    "\n",
    "            # Draw bounding boxes\n",
    "            for i in range(output.shape[1]):\n",
    "                detection = output[:, i]\n",
    "                x_center, y_center, width, height = detection[:4]\n",
    "                x1 = max(0, int((x_center - width / 2) * orig_w))\n",
    "                y1 = max(0, int((y_center - height / 2) * orig_h))\n",
    "                x2 = min(orig_w - 1, int((x_center + width / 2) * orig_w))\n",
    "                y2 = min(orig_h - 1, int((y_center + height / 2) * orig_h))\n",
    "\n",
    "                score = np.max(detection[4:])\n",
    "                cls = np.argmax(detection[4:])\n",
    "                if score >= threshold:\n",
    "                    class_name = class_labels[str(cls)]  # Obtain the class name using the class index\n",
    "                    # Use lock to ensure safe access to frame\n",
    "                    with lock:\n",
    "                        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), thickness=2)  # Draw bounding box\n",
    "                        cv2.putText(frame, f\"{class_name}: {score:.2f}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
    "                                    (0, 255, 0), 2)\n",
    "\n",
    "            # Display the processed frame\n",
    "            with lock:\n",
    "                cv2.imshow('Object Detection', frame)\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    break\n",
    "\n",
    "    # Calculate FPS\n",
    "    end_time = cv2.getTickCount()\n",
    "    total_time = (end_time - start_time) / cv2.getTickFrequency()  # in seconds\n",
    "    fps = frame_count / total_time\n",
    "    print(f\"Average FPS: {fps}\")\n",
    "\n",
    "# Function to open a file dialog and select a video\n",
    "def open_video():\n",
    "    file_path = filedialog.askopenfilename()\n",
    "    if file_path:\n",
    "        cap = cv2.VideoCapture(file_path)\n",
    "        # Start a new thread for preprocessing\n",
    "        threading.Thread(target=preprocessing_thread, args=(cap,), daemon=True).start()\n",
    "        # Start the inference thread\n",
    "        threading.Thread(target=inference_thread, daemon=True).start()\n",
    "\n",
    "# Button to select a video\n",
    "select_button = tk.Button(root, text=\"Select Video\", command=open_video)\n",
    "select_button.pack()\n",
    "\n",
    "# Run the tkinter event loop\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Input Dimensions: [224 224   3]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tkinter import filedialog\n",
    "import json\n",
    "import os\n",
    "from tflite_runtime.interpreter import Interpreter, load_delegate\n",
    "import threading\n",
    "\n",
    "# RTSP URL\n",
    "rtsp_url = 'rtsp://admin:admin@192.168.1.11:1935'\n",
    "\n",
    "# Load the TensorFlow Lite model with Edge TPU delegate\n",
    "model_path = 'yolov8m_saved_model/yolov8m_integer_quant.tflite'\n",
    "interpreter = Interpreter(model_path=model_path, experimental_delegates=[load_delegate('edgetpu.dll')])\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "input_shape = input_details[0]['shape'][1:]\n",
    "print(\"Expected Input Dimensions:\", input_details[0]['shape'][1:])\n",
    "\n",
    "# Threshold Setting\n",
    "threshold = 0.3\n",
    "\n",
    "# Load the class labels from the JSON file\n",
    "with open('C:/object-detection-coral/label_files/labels_coco.json', 'r') as json_file:\n",
    "    class_labels = json.load(json_file)\n",
    "\n",
    "# Create a tkinter window\n",
    "root = tk.Tk()\n",
    "root.title(\"Object Detection\")\n",
    "\n",
    "# save_image_with_boxes\n",
    "output_directory = \"output_images\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Function to preprocess the frame\n",
    "def preprocess(frame, input_shape):\n",
    "    # Resize frame\n",
    "    image_resized = cv2.resize(frame, (input_shape[0], input_shape[1]))\n",
    "    # Convert image to float32\n",
    "    image_float32 = image_resized.astype(np.float32)\n",
    "    # Normalize image\n",
    "    image_normalized = image_float32 / 255.0\n",
    "    # Add batch dimension\n",
    "    image_with_batch = np.expand_dims(image_normalized, 0)\n",
    "\n",
    "    return image_with_batch\n",
    "\n",
    "# Function to perform inference on frames\n",
    "def inference_thread(cap):\n",
    "    frame_count = 0\n",
    "    start_time = cv2.getTickCount()  # Get initial time\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Increment frame count\n",
    "        frame_count += 1\n",
    "\n",
    "        # Preprocess the frame\n",
    "        image_np = preprocess(frame, input_shape)\n",
    "\n",
    "        # Perform inference\n",
    "        interpreter.set_tensor(input_details[0]['index'], image_np)\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details[0]['index'])\n",
    "        output = output[0]\n",
    "\n",
    "        # Draw bounding boxes\n",
    "        orig_h, orig_w = frame.shape[:2]\n",
    "        for i in range(output.shape[1]):\n",
    "            detection = output[:, i]\n",
    "            x_center, y_center, width, height = detection[:4]\n",
    "            x1 = max(0, int((x_center - width / 2) * orig_w))\n",
    "            y1 = max(0, int((y_center - height / 2) * orig_h))\n",
    "            x2 = min(orig_w - 1, int((x_center + width / 2) * orig_w))\n",
    "            y2 = min(orig_h - 1, int((y_center + height / 2) * orig_h))\n",
    "            \n",
    "            score = np.max(detection[4:])\n",
    "            cls = np.argmax(detection[4:])\n",
    "            if score >= threshold:\n",
    "                class_name = class_labels[str(cls)]  # Obtain the class name using the class index\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), thickness=2)  # Draw bounding box\n",
    "                cv2.putText(frame, f\"{class_name}: {score:.2f}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "        # Display the processed frame\n",
    "        cv2.imshow('Object Detection', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Calculate FPS\n",
    "    end_time = cv2.getTickCount()\n",
    "    total_time = (end_time - start_time) / cv2.getTickFrequency() # in seconds\n",
    "    fps = frame_count / total_time\n",
    "\n",
    "    print(f\"Average FPS: {fps}\")\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Function to open RTSP stream and start inference thread\n",
    "def open_rtsp_stream():\n",
    "    cap = cv2.VideoCapture(rtsp_url)\n",
    "    # Start a new thread for inference\n",
    "    threading.Thread(target=inference_thread, args=(cap,), daemon=True).start()\n",
    "\n",
    "# Button to open RTSP stream\n",
    "select_button = tk.Button(root, text=\"Open RTSP Stream\", command=open_rtsp_stream)\n",
    "select_button.pack()\n",
    "\n",
    "# Run the tkinter event loop\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Input Dimensions: [640 640   3]\n",
      "Average FPS: 41.91746360451687\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tkinter as tk\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageTk\n",
    "from tkinter import filedialog\n",
    "import json\n",
    "import os\n",
    "from tflite_runtime.interpreter import Interpreter, load_delegate\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Load the TensorFlow Lite model with Edge TPU delegate\n",
    "model_path = 'yolov8m_saved_model/yolov8m_integer_quant.tflite'\n",
    "interpreter = Interpreter(model_path=model_path, experimental_delegates=[load_delegate('edgetpu.dll')])\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "input_shape = input_details[0]['shape'][1:]\n",
    "print(\"Expected Input Dimensions:\", input_details[0]['shape'][1:])\n",
    "\n",
    "# Threshold Setting\n",
    "threshold = 0.5\n",
    "\n",
    "# Load the class labels from the JSON file\n",
    "with open('C:/object-detection-coral/label_files/labels_coco.json', 'r') as json_file:\n",
    "    class_labels = json.load(json_file)\n",
    "\n",
    "# Create a tkinter window\n",
    "root = tk.Tk()\n",
    "root.title(\"Object Detection\")\n",
    "\n",
    "# Save image with boxes\n",
    "output_directory = \"output_images\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "def save_image_with_boxes(image, file_path):\n",
    "    output_path = os.path.join(output_directory, os.path.basename(file_path))\n",
    "    cv2.imwrite(output_path, image)\n",
    "    print(f\"Image saved successfully: {output_path}\")\n",
    "\n",
    "# Pre-allocate buffer for resized image\n",
    "buffer = np.empty(input_shape, dtype=np.float32)\n",
    "\n",
    "# Function to preprocess the frame\n",
    "def preprocess(frame, input_shape, buffer):\n",
    "    # Resize frame and store it in the pre-allocated buffer\n",
    "    cv2.resize(frame, (input_shape[0], input_shape[1]), dst=buffer)\n",
    "    # Convert image to float32\n",
    "    buffer_normalized = buffer / 255.0\n",
    "    # Add batch dimension\n",
    "    image_with_batch = np.expand_dims(buffer_normalized, 0)\n",
    "\n",
    "    return image_with_batch\n",
    "\n",
    "# Function to process video for object detection\n",
    "def process_video(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    start_time = cv2.getTickCount()  # Get initial time\n",
    "    frame_count = 0\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Increment frame count\n",
    "            frame_count += 1\n",
    "\n",
    "            # Preprocess the frame asynchronously\n",
    "            future = executor.submit(preprocess, frame.copy(), input_shape, buffer.copy())\n",
    "            futures.append((future, frame))\n",
    "\n",
    "        # Wait for all preprocessing to finish\n",
    "        for future, frame in futures:\n",
    "            image_np = future.result()\n",
    "\n",
    "            # Perform inference\n",
    "            interpreter.set_tensor(input_details[0]['index'], image_np)\n",
    "            interpreter.invoke()\n",
    "            output = interpreter.get_tensor(output_details[0]['index'])\n",
    "            output = output[0]\n",
    "\n",
    "            # Draw bounding boxes\n",
    "            # Draw bounding boxes and print predictions\n",
    "            # Draw bounding boxes and print predictions\n",
    "            orig_h, orig_w = frame.shape[:2]\n",
    "            for i in range(output.shape[1]):\n",
    "                detection = output[:, i]\n",
    "                x_center, y_center, width, height = detection[:4]\n",
    "                x1 = max(0, int((x_center - width / 2) * orig_w))\n",
    "                y1 = max(0, int((y_center - height / 2) * orig_h))\n",
    "                x2 = min(orig_w - 1, int((x_center + width / 2) * orig_w))\n",
    "                y2 = min(orig_h - 1, int((y_center + height / 2) * orig_h))\n",
    "\n",
    "                score = np.max(detection[4:])\n",
    "                cls = np.argmax(detection[4:])\n",
    "                if score >= threshold:\n",
    "                    class_name = class_labels[str(cls)]  # Obtain the class name using the class index\n",
    "                    # Draw bounding box\n",
    "                    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), thickness=2)\n",
    "                    # Put text with class name and score\n",
    "                    text = f\"{class_name}: {score:.2f}\"\n",
    "                    cv2.putText(frame, text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "                    # Print prediction\n",
    "                    print(f\"Detected: {class_name} with confidence {score:.2f}\")\n",
    "\n",
    "\n",
    "            # Display the processed frame\n",
    "            cv2.imshow('Object Detection', frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    # Calculate FPS\n",
    "    end_time = cv2.getTickCount()\n",
    "    total_time = (end_time - start_time) / cv2.getTickFrequency()  # in seconds\n",
    "    fps = frame_count / total_time\n",
    "\n",
    "    print(f\"Average FPS: {fps}\")\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Function to open a file dialog and select a video\n",
    "def open_video():\n",
    "    file_path = filedialog.askopenfilename()\n",
    "    if file_path:\n",
    "        process_video(file_path)\n",
    "\n",
    "# Button to select a video\n",
    "select_button = tk.Button(root, text=\"Select Video\", command=open_video)\n",
    "select_button.pack()\n",
    "\n",
    "# Run the tkinter event loop\n",
    "root.mainloop()\n",
    "\n",
    "\n",
    "#This code incorporates the optimizations mentioned earlier and should improve the performance of your object detection application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Input Dimensions: [224 224   3]\n",
      "Average FPS: 9.439081266301942\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tkinter import filedialog\n",
    "import json\n",
    "import os\n",
    "from tflite_runtime.interpreter import Interpreter, load_delegate\n",
    "\n",
    "# Load the TensorFlow Lite model with Edge TPU delegate\n",
    "model_path = 'yolov8m_saved_model/yolov8m_integer_quant.tflite'\n",
    "interpreter = Interpreter(model_path=model_path, experimental_delegates=[load_delegate('edgetpu.dll')])\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "input_shape = input_details[0]['shape'][1:]\n",
    "print(\"Expected Input Dimensions:\", input_details[0]['shape'][1:])\n",
    "\n",
    "# Threshold Setting\n",
    "threshold = 0.5\n",
    "\n",
    "# Load the class labels from the JSON file\n",
    "with open('C:/object-detection-coral/label_files/labels_coco.json', 'r') as json_file:\n",
    "    class_labels = json.load(json_file)\n",
    "\n",
    "# Create a tkinter window\n",
    "root = tk.Tk()\n",
    "root.title(\"Object Detection\")\n",
    "\n",
    "# save_image_with_boxes\n",
    "output_directory = \"output_images\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "def save_image_with_boxes(image, file_path):\n",
    "    output_path = os.path.join(output_directory, os.path.basename(file_path))\n",
    "    cv2.imwrite(output_path, image)\n",
    "    print(f\"Image saved successfully: {output_path}\")\n",
    "\n",
    "# Function to process video for object detection\n",
    "def process_video(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    start_time = cv2.getTickCount() # Get initial time\n",
    "    frame_count = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Increment frame count\n",
    "        frame_count += 1\n",
    "\n",
    "        # Resize the frame to match the input dimensions\n",
    "        frame_resized = cv2.resize(frame, (input_shape[0], input_shape[1]))\n",
    "\n",
    "        # Preallocate memory for the resized frame and normalized image\n",
    "        image_float32 = np.empty_like(frame_resized, dtype=np.float32)\n",
    "        image_normalized = np.empty_like(image_float32)\n",
    "\n",
    "        # Convert image to float32 and normalize\n",
    "        cv2.normalize(frame_resized, image_float32, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "        image_with_batch = np.expand_dims(image_float32, 0)\n",
    "\n",
    "        # Perform inference\n",
    "        interpreter.set_tensor(input_details[0]['index'], image_with_batch)\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details[0]['index'])\n",
    "        output = output[0]\n",
    "\n",
    "        # Draw bounding boxes\n",
    "        orig_h, orig_w = frame.shape[:2]\n",
    "        for i in range(output.shape[1]):\n",
    "            detection = output[:, i]\n",
    "            x_center, y_center, width, height = detection[:4]\n",
    "            x1 = max(0, int((x_center - width / 2) * orig_w))\n",
    "            y1 = max(0, int((y_center - height / 2) * orig_h))\n",
    "            x2 = min(orig_w - 1, int((x_center + width / 2) * orig_w))\n",
    "            y2 = min(orig_h - 1, int((y_center + height / 2) * orig_h))\n",
    "            \n",
    "            score = np.max(detection[4:])\n",
    "            cls = np.argmax(detection[4:])\n",
    "            if score >= threshold:\n",
    "                class_name = class_labels[str(cls)]  # Obtain the class name using the class index\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), thickness=2)  # Draw bounding box\n",
    "                cv2.putText(frame, f\"{class_name}: {score:.2f}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "        # Display the processed frame\n",
    "        cv2.imshow('Object Detection', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Calculate FPS\n",
    "    end_time = cv2.getTickCount()\n",
    "    total_time = (end_time - start_time) / cv2.getTickFrequency() # in seconds\n",
    "    fps = frame_count / total_time\n",
    "\n",
    "    print(f\"Average FPS: {fps}\")\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Function to open a file dialog and select a video\n",
    "def open_video():\n",
    "    file_path = filedialog.askopenfilename()\n",
    "    if file_path:\n",
    "        process_video(file_path)\n",
    "\n",
    "# Button to select a video\n",
    "select_button = tk.Button(root, text=\"Select Video\", command=open_video)\n",
    "select_button.pack()\n",
    "\n",
    "# Run the tkinter event loop\n",
    "root.mainloop()\n",
    "#s7i7a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Input Dimensions: [224 224   3]\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageTk\n",
    "from tkinter import filedialog\n",
    "import json\n",
    "import os\n",
    "from tflite_runtime.interpreter import Interpreter, load_delegate\n",
    "import time\n",
    "\n",
    "# Load the TensorFlow Lite model with Edge TPU delegate\n",
    "model_path = 'yolov8m_saved_model/yolov8m_float32.tflite'\n",
    "interpreter = Interpreter(model_path=model_path, experimental_delegates=[load_delegate('edgetpu.dll')])\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "input_shape = input_details[0]['shape'][1:]\n",
    "print(\"Expected Input Dimensions:\", input_details[0]['shape'][1:])\n",
    "\n",
    "# Threshold Setting\n",
    "threshold = 0.5\n",
    "\n",
    "# Load the class labels from the JSON file\n",
    "with open('C:\\object-detection-coral\\label_files\\labels_coco.json', 'r') as json_file:\n",
    "    class_labels = json.load(json_file)\n",
    "\n",
    "# Create a tkinter window\n",
    "root = tk.Tk()\n",
    "root.title(\"Object Detection\")\n",
    "\n",
    "# save_image_with_boxes\n",
    "output_directory = \"output_images\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "def save_image_with_boxes(image, file_path):\n",
    "    output_path = os.path.join(output_directory, os.path.basename(file_path))\n",
    "    cv2.imwrite(output_path, image)\n",
    "    print(f\"Image saved successfully: {output_path}\")\n",
    "\n",
    "# Function to preprocess the frame\n",
    "def preprocess(frame, input_shape):\n",
    "    # Resize frame\n",
    "    image_resized = cv2.resize(frame, (input_shape[0], input_shape[1]))\n",
    "    # Convert image to float32\n",
    "    image_float32 = image_resized.astype(np.float32)\n",
    "    # Normalize image\n",
    "    image_normalized = image_float32 / 255.0\n",
    "    # Add batch dimension\n",
    "    image_with_batch = np.expand_dims(image_normalized, 0)\n",
    "\n",
    "    return image_with_batch\n",
    "\n",
    "# Function to process video for object detection\n",
    "def process_video(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    prev_frame_time = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Preprocess the frame\n",
    "        image_np = preprocess(frame, input_shape)\n",
    "\n",
    "        # Perform inference\n",
    "        interpreter.set_tensor(input_details[0]['index'], image_np)\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details[0]['index'])\n",
    "        output = output[0]\n",
    "\n",
    "        # Draw bounding boxes\n",
    "        orig_h, orig_w = frame.shape[:2]\n",
    "        for i in range(output.shape[1]):\n",
    "            detection = output[:, i]\n",
    "            x_center, y_center, width, height = detection[:4]\n",
    "            x1 = max(0, int((x_center - width / 2) * orig_w))\n",
    "            y1 = max(0, int((y_center - height / 2) * orig_h))\n",
    "            x2 = min(orig_w - 1, int((x_center + width / 2) * orig_w))\n",
    "            y2 = min(orig_h - 1, int((y_center + height / 2) * orig_h))\n",
    "            \n",
    "            score = np.max(detection[4:])\n",
    "            cls = np.argmax(detection[4:])\n",
    "            if score >= threshold:\n",
    "                class_name = class_labels[str(cls)]  # Obtain the class name using the class index\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), thickness=2)  # Draw bounding box\n",
    "                cv2.putText(frame, f\"{class_name}: {score:.2f}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "        # Calculate FPS\n",
    "        new_frame_time = time.time()\n",
    "        fps = 1 / (new_frame_time - prev_frame_time)\n",
    "        prev_frame_time = new_frame_time\n",
    "\n",
    "        # Convert the FPS to string for display\n",
    "        fps_str = f\"FPS: {int(fps)}\"\n",
    "       \n",
    "        \n",
    "        # Put FPS text on the frame\n",
    "        cv2.putText(frame, fps_str, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "        # Display the processed frame\n",
    "        cv2.imshow('Object Detection', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Function to open a file dialog and select a video\n",
    "def open_video():\n",
    "    file_path = filedialog.askopenfilename()\n",
    "    if file_path:\n",
    "        process_video(file_path)\n",
    "\n",
    "# Button to select a video\n",
    "select_button = tk.Button(root, text=\"Select Video\", command=open_video)\n",
    "select_button.pack()\n",
    "\n",
    "# Run the tkinter event loop\n",
    "root.mainloop()\n",
    "#as7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Input Dimensions: [224 224   3]\n",
      "Average FPS: 4.417821208809997\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tkinter import filedialog\n",
    "import json\n",
    "import os\n",
    "from tflite_runtime.interpreter import Interpreter, load_delegate\n",
    "import threading\n",
    "\n",
    "# Load the TensorFlow Lite model with Edge TPU delegate\n",
    "model_path = 'yolov8m_saved_model/yolov8m_integer_quant.tflite'\n",
    "interpreter = Interpreter(model_path=model_path, experimental_delegates=[load_delegate('edgetpu.dll')])\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "input_shape = input_details[0]['shape'][1:]\n",
    "print(\"Expected Input Dimensions:\", input_details[0]['shape'][1:])\n",
    "\n",
    "# Threshold Setting\n",
    "threshold = 0.3\n",
    "\n",
    "# Load the class labels from the JSON file\n",
    "with open('C:/object-detection-coral/label_files/labels_coco.json', 'r') as json_file:\n",
    "    class_labels = json.load(json_file)\n",
    "\n",
    "# Create a tkinter window\n",
    "root = tk.Tk()\n",
    "root.title(\"Object Detection\")\n",
    "\n",
    "# save_image_with_boxes\n",
    "\"\"\"output_directory = \"output_images\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "def save_image_with_boxes(image, file_path):\n",
    "    output_path = os.path.join(output_directory, os.path.basename(file_path))\n",
    "    cv2.imwrite(output_path, image)\n",
    "    print(f\"Image saved successfully: {output_path}\")\"\"\"\n",
    "\n",
    "# Function to process video frames in a separate thread\n",
    "def process_video_thread(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = 0\n",
    "    start_time = cv2.getTickCount()  # Define start time here\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Increment frame count\n",
    "        frame_count += 1\n",
    "\n",
    "        # Resize the frame to match the input dimensions\n",
    "        frame_resized = cv2.resize(frame, (input_shape[0], input_shape[1]))\n",
    "\n",
    "        # Preallocate memory for the resized frame and normalized image\n",
    "        image_float32 = np.empty_like(frame_resized, dtype=np.float32)\n",
    "        image_normalized = np.empty_like(image_float32)\n",
    "\n",
    "        # Convert image to float32 and normalize\n",
    "        cv2.normalize(frame_resized, image_float32, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "        image_with_batch = np.expand_dims(image_float32, 0)\n",
    "\n",
    "        # Perform inference\n",
    "        interpreter.set_tensor(input_details[0]['index'], image_with_batch)\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details[0]['index'])\n",
    "        output = output[0]\n",
    "\n",
    "        # Draw bounding boxes\n",
    "        orig_h, orig_w = frame.shape[:2]\n",
    "        for i in range(output.shape[1]):\n",
    "            detection = output[:, i]\n",
    "            x_center, y_center, width, height = detection[:4]\n",
    "            x1 = max(0, int((x_center - width / 2) * orig_w))\n",
    "            y1 = max(0, int((y_center - height / 2) * orig_h))\n",
    "            x2 = min(orig_w - 1, int((x_center + width / 2) * orig_w))\n",
    "            y2 = min(orig_h - 1, int((y_center + height / 2) * orig_h))\n",
    "            \n",
    "            score = np.max(detection[4:])\n",
    "            cls = np.argmax(detection[4:])\n",
    "            if score >= threshold:\n",
    "                class_name = class_labels[str(cls)]  # Obtain the class name using the class index\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), thickness=2)  # Draw bounding box\n",
    "                cv2.putText(frame, f\"{class_name}: {score:.2f}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "        # Display the processed frame\n",
    "        cv2.imshow('Object Detection', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Calculate FPS\n",
    "    end_time = cv2.getTickCount()\n",
    "    total_time = (end_time - start_time) / cv2.getTickFrequency() # in seconds\n",
    "    fps = frame_count / total_time\n",
    "\n",
    "    print(f\"Average FPS: {fps}\")\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Function to process video\n",
    "def process_video(video_path):\n",
    "    thread = threading.Thread(target=process_video_thread, args=(video_path,))\n",
    "    thread.start()\n",
    "\n",
    "# Function to open a file dialog and select a video\n",
    "def open_video():\n",
    "    file_path = filedialog.askopenfilename()\n",
    "    if file_path:\n",
    "        process_video(file_path)\n",
    "\n",
    "# Button to select a video\n",
    "select_button = tk.Button(root, text=\"Select Video\", command=open_video)\n",
    "select_button.pack()\n",
    "\n",
    "# Run the tkinter event loop\n",
    "root.mainloop()\n",
    "#metwasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Input Dimensions: [224 224   3]\n",
      "Average FPS: 3.2611326597147166\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Input Dimensions: [224 224   3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-8 (inference_thread):\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python311\\Lib\\threading.py\", line 1038, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\object-detection-coral\\venv-inference\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 761, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"C:\\Python311\\Lib\\threading.py\", line 975, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\Ala Arboun\\AppData\\Local\\Temp\\ipykernel_18772\\1479945838.py\", line 62, in inference_thread\n",
      "  File \"c:\\object-detection-coral\\venv-inference\\Lib\\site-packages\\tflite_runtime\\interpreter.py\", line 940, in invoke\n",
      "    self._ensure_safe()\n",
      "  File \"c:\\object-detection-coral\\venv-inference\\Lib\\site-packages\\tflite_runtime\\interpreter.py\", line 556, in _ensure_safe\n",
      "    raise RuntimeError(\"\"\"There is at least 1 reference to internal data\n",
      "RuntimeError: There is at least 1 reference to internal data\n",
      "      in the interpreter in the form of a numpy array or slice. Be sure to\n",
      "      only hold the function returned from tensor() if you are using raw\n",
      "      data access.\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tkinter import filedialog\n",
    "import json\n",
    "import os\n",
    "from tflite_runtime.interpreter import Interpreter, load_delegate\n",
    "import threading\n",
    "\n",
    "\n",
    "model_path = 'yolov8m_saved_model/yolov8m_integer_quant.tflite' # Load the TensorFlow Lite model with Edge TPU delegate\n",
    "interpreter = Interpreter(model_path=model_path, experimental_delegates=[load_delegate('edgetpu.dll')])\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "input_shape = input_details[0]['shape'][1:]\n",
    "print(\"Expected Input Dimensions:\", input_details[0]['shape'][1:])\n",
    "\n",
    "threshold = 0.3\n",
    "\n",
    "\n",
    "with open('C:/object-detection-coral/label_files/labels_coco.json', 'r') as json_file:\n",
    "    class_labels = json.load(json_file)\n",
    "\n",
    "\n",
    "root = tk.Tk()\n",
    "root.title(\"Object Detection\")\n",
    "\n",
    "output_directory = \"output_images\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "\n",
    "def preprocess(frame, input_shape):\n",
    "    # Resize frame\n",
    "    image_resized = cv2.resize(frame, (input_shape[0], input_shape[1]))\n",
    "    # Convert image to float32\n",
    "    image_float32 = image_resized.astype(np.float32)\n",
    "    # Normalize image\n",
    "    image_normalized = image_float32 / 255.0\n",
    "    # Add batch dimension\n",
    "    image_with_batch = np.expand_dims(image_normalized, 0)\n",
    "\n",
    "    return image_with_batch\n",
    "\n",
    "def inference_thread(cap):\n",
    "    frame_count = 0\n",
    "    start_time = cv2.getTickCount()  # Get initial time\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Increment frame count\n",
    "        frame_count += 1\n",
    "\n",
    "        # Preprocess the frame\n",
    "        image_np = preprocess(frame, input_shape)\n",
    "\n",
    "        # Perform inference\n",
    "        interpreter.set_tensor(input_details[0]['index'], image_np)\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details[0]['index'])\n",
    "        output = output[0]\n",
    "\n",
    "        # Draw bounding boxes\n",
    "        orig_h, orig_w = frame.shape[:2]\n",
    "        for i in range(output.shape[1]):\n",
    "            detection = output[:, i]\n",
    "            x_center, y_center, width, height = detection[:4]\n",
    "            x1 = max(0, int((x_center - width / 2) * orig_w))\n",
    "            y1 = max(0, int((y_center - height / 2) * orig_h))\n",
    "            x2 = min(orig_w - 1, int((x_center + width / 2) * orig_w))\n",
    "            y2 = min(orig_h - 1, int((y_center + height / 2) * orig_h))\n",
    "            \n",
    "            score = np.max(detection[4:])\n",
    "            cls = np.argmax(detection[4:])\n",
    "            if score >= threshold:\n",
    "                class_name = class_labels[str(cls)]  # Obtain the class name using the class index\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), thickness=2)  # Draw bounding box\n",
    "                cv2.putText(frame, f\"{class_name}: {score:.2f}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "        # Display the processed frame\n",
    "        cv2.imshow('Object Detection', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "\n",
    "    end_time = cv2.getTickCount()\n",
    "    total_time = (end_time - start_time) / cv2.getTickFrequency() # in seconds\n",
    "    fps = frame_count / total_time\n",
    "\n",
    "    print(f\"Average FPS: {fps}\")\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "def open_video():\n",
    "    file_path = filedialog.askopenfilename()\n",
    "    if file_path:\n",
    "        cap = cv2.VideoCapture(file_path)\n",
    "        # Start a new thread for inference\n",
    "        threading.Thread(target=inference_thread, args=(cap,), daemon=True).start()\n",
    "\n",
    "select_button = tk.Button(root, text=\"Select Video\", command=open_video)\n",
    "select_button.pack()\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average FPS: 8.343751434082277\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tkinter import filedialog\n",
    "import json\n",
    "import os\n",
    "from tflite_runtime.interpreter import Interpreter, load_delegate\n",
    "import threading\n",
    "from queue import Queue\n",
    "\n",
    "# Load the TensorFlow Lite model with Edge TPU delegate\n",
    "model_path = 'yolov8m_saved_model/yolov8m_integer_quant.tflite'\n",
    "interpreter = Interpreter(model_path=model_path, experimental_delegates=[load_delegate('edgetpu.dll')])\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "input_shape = input_details[0]['shape'][1:]\n",
    "#print(\"Expected Input Dimensions:\", input_details[0]['shape'][1:])\n",
    "\n",
    "# Threshold Setting\n",
    "threshold = 0.3\n",
    "\n",
    "# Load the class labels from the JSON file\n",
    "with open('C:/object-detection-coral/label_files/labels_coco.json', 'r') as json_file:\n",
    "    class_labels = json.load(json_file)\n",
    "\n",
    "# Create a tkinter window\n",
    "root = tk.Tk()\n",
    "root.title(\"Object Detection\")\n",
    "\n",
    "# save_image_with_boxes\n",
    "output_directory = \"output_images\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Queue for preprocessed frames\n",
    "preprocessed_queue = Queue(maxsize=5)  # Adjust the size according to your needs\n",
    "\n",
    "# Lock for synchronization\n",
    "lock = threading.Lock()\n",
    "\n",
    "# Function to preprocess the frame\n",
    "def preprocess_frame(frame, input_shape):\n",
    "    # Resize frame\n",
    "    image_resized = cv2.resize(frame, (input_shape[0], input_shape[1]))\n",
    "    # Convert image to float32\n",
    "    image_float32 = image_resized.astype(np.float32)\n",
    "    # Normalize image\n",
    "    image_normalized = image_float32 / 255.0\n",
    "    # Add batch dimension\n",
    "    image_with_batch = np.expand_dims(image_normalized, 0)\n",
    "    return image_with_batch\n",
    "\n",
    "# Preprocessing thread function\n",
    "def preprocessing_thread(cap):\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # Preprocess the frame\n",
    "        preprocessed_frame = preprocess_frame(frame, input_shape)\n",
    "        # Get original frame dimensions\n",
    "        orig_h, orig_w = frame.shape[:2]\n",
    "        # Put the preprocessed frame and its dimensions into the queue\n",
    "        preprocessed_queue.put((preprocessed_frame, frame, orig_h, orig_w))\n",
    "    cap.release()\n",
    "\n",
    "# Function to perform inference on frames\n",
    "def inference_thread():\n",
    "    frame_count = 0\n",
    "    start_time = cv2.getTickCount()  # Get initial time\n",
    "    while True:\n",
    "        if not preprocessed_queue.empty():\n",
    "            # Get a preprocessed frame from the queue\n",
    "            preprocessed_frame, frame, orig_h, orig_w = preprocessed_queue.get()\n",
    "            frame_count += 1\n",
    "\n",
    "            # Perform inference\n",
    "            interpreter.set_tensor(input_details[0]['index'], preprocessed_frame)\n",
    "            interpreter.invoke()\n",
    "            output = interpreter.get_tensor(output_details[0]['index'])\n",
    "            output = output[0]\n",
    "            \n",
    "            \n",
    "\n",
    "            # Draw bounding boxes\n",
    "            for i in range(output.shape[1]):\n",
    "                detection = output[:, i]\n",
    "                x_center, y_center, width, height = detection[:4]\n",
    "                x1 = max(0, int((x_center - width / 2) * orig_w))\n",
    "                y1 = max(0, int((y_center - height / 2) * orig_h))\n",
    "                x2 = min(orig_w - 1, int((x_center + width / 2) * orig_w))\n",
    "                y2 = min(orig_h - 1, int((y_center + height / 2) * orig_h))\n",
    "\n",
    "                score = np.max(detection[4:])\n",
    "                cls = np.argmax(detection[4:])\n",
    "                if score >= threshold:\n",
    "                    class_name = class_labels[str(cls)]  # Obtain the class name using the class index\n",
    "                    # Use lock to ensure safe access to frame\n",
    "                    with lock:\n",
    "                        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), thickness=2)  # Draw bounding box\n",
    "                        cv2.putText(frame, f\"{class_name}: {score:.2f}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
    "                                    (0, 255, 0), 2)\n",
    "\n",
    "            # Display the processed frame\n",
    "            with lock:\n",
    "                cv2.imshow('Object Detection', frame)\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    break\n",
    "\n",
    "    # Calculate FPS\n",
    "    end_time = cv2.getTickCount()\n",
    "    total_time = (end_time - start_time) / cv2.getTickFrequency()  # in seconds\n",
    "    fps = frame_count / total_time\n",
    "    print(f\"Average FPS: {fps}\")\n",
    "\n",
    "# Function to open a file dialog and select a video\n",
    "def open_video():\n",
    "    file_path = filedialog.askopenfilename()\n",
    "    if file_path:\n",
    "        cap = cv2.VideoCapture(file_path)\n",
    "        # Start a new thread for preprocessing\n",
    "        threading.Thread(target=preprocessing_thread, args=(cap,), daemon=True).start()\n",
    "        # Start the inference thread\n",
    "        threading.Thread(target=inference_thread, daemon=True).start()\n",
    "\n",
    "# Button to select a video\n",
    "select_button = tk.Button(root, text=\"Select Video\", command=open_video)\n",
    "select_button.pack()\n",
    "\n",
    "# Run the tkinter event loop\n",
    "root.mainloop()\n",
    "\n",
    "#optimized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average FPS: 13.156534111868378\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tkinter import filedialog\n",
    "import json\n",
    "import os\n",
    "from tflite_runtime.interpreter import Interpreter, load_delegate\n",
    "import threading\n",
    "from queue import Queue\n",
    "\n",
    "# Load the TensorFlow Lite model with Edge TPU delegate\n",
    "model_path = 'yolov8m_saved_model/yolov8m_integer_quant.tflite'\n",
    "interpreter = Interpreter(model_path=model_path, experimental_delegates=[load_delegate('edgetpu.dll')])\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "input_shape = input_details[0]['shape'][1:]\n",
    "#print(\"Expected Input Dimensions:\", input_details[0]['shape'][1:])\n",
    "\n",
    "# Threshold Setting\n",
    "threshold = 0.3\n",
    "\n",
    "# Load the class labels from the JSON file\n",
    "with open('C:/object-detection-coral/label_files/labels_coco.json', 'r') as json_file:\n",
    "    class_labels = json.load(json_file)\n",
    "\n",
    "# Create a tkinter window\n",
    "root = tk.Tk()\n",
    "root.title(\"Object Detection\")\n",
    "\n",
    "# save_image_with_boxes\n",
    "output_directory = \"output_images\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Queue for preprocessed frames\n",
    "preprocessed_queue = Queue(maxsize=5)  # Adjust the size according to your needs\n",
    "\n",
    "# Lock for synchronization\n",
    "lock = threading.Lock()\n",
    "\n",
    "# Function to preprocess the frame\n",
    "def preprocess_frame(frame, input_shape):\n",
    "    # Resize frame\n",
    "    image_resized = cv2.resize(frame, (input_shape[0], input_shape[1]))\n",
    "    # Convert image to float32\n",
    "    image_float32 = image_resized.astype(np.float32)\n",
    "    # Normalize image\n",
    "    image_normalized = image_float32 / 255.0\n",
    "    # Add batch dimension\n",
    "    image_with_batch = np.expand_dims(image_normalized, 0)\n",
    "    return image_with_batch\n",
    "\n",
    "# Preprocessing thread function\n",
    "def preprocessing_thread(cap):\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # Preprocess the frame\n",
    "        preprocessed_frame = preprocess_frame(frame, input_shape)\n",
    "        # Get original frame dimensions\n",
    "        orig_h, orig_w = frame.shape[:2]\n",
    "        # Put the preprocessed frame and its dimensions into the queue\n",
    "        preprocessed_queue.put((preprocessed_frame, frame, orig_h, orig_w))\n",
    "    cap.release()\n",
    "\n",
    "# Define the Non-Maximum Suppression (NMS) function\n",
    "def nms(output, threshold):\n",
    "    if len(output) == 0:\n",
    "        return []\n",
    "\n",
    "    # Extract bounding boxes and confidence scores from output\n",
    "    boxes = output[:, :4]\n",
    "    scores = np.max(output[:, 4:], axis=1)\n",
    "\n",
    "    # Compute the area of the bounding boxes\n",
    "    areas = (boxes[:, 2] - boxes[:, 0] + 1) * (boxes[:, 3] - boxes[:, 1] + 1)\n",
    "\n",
    "    # Sort by confidence scores in descending order\n",
    "    order = scores.argsort()[::-1]\n",
    "\n",
    "    # Initialize list to store selected indices after NMS\n",
    "    selected_indices = []\n",
    "\n",
    "    while len(order) > 0:\n",
    "        # Select the bounding box with the highest confidence score\n",
    "        selected_index = order[0]\n",
    "        selected_indices.append(selected_index)\n",
    "\n",
    "        # Compute IoU (Intersection over Union) with other bounding boxes\n",
    "        xx1 = np.maximum(boxes[selected_index, 0], boxes[order[1:], 0])\n",
    "        yy1 = np.maximum(boxes[selected_index, 1], boxes[order[1:], 1])\n",
    "        xx2 = np.minimum(boxes[selected_index, 2], boxes[order[1:], 2])\n",
    "        yy2 = np.minimum(boxes[selected_index, 3], boxes[order[1:], 3])\n",
    "\n",
    "        w = np.maximum(0.0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0.0, yy2 - yy1 + 1)\n",
    "        overlap = (w * h) / (areas[selected_index] + areas[order[1:]] - w * h)\n",
    "\n",
    "        # Remove bounding boxes with IoU greater than the threshold\n",
    "        inds_to_keep = np.where(overlap <= threshold)[0]\n",
    "        order = order[inds_to_keep + 1]\n",
    "\n",
    "    return selected_indices\n",
    "\n",
    "# Function to perform inference on frames\n",
    "def inference_thread():\n",
    "    frame_count = 0\n",
    "    start_time = cv2.getTickCount()  # Get initial time\n",
    "    while True:\n",
    "        if not preprocessed_queue.empty():\n",
    "            # Get a preprocessed frame from the queue\n",
    "            preprocessed_frame, frame, orig_h, orig_w = preprocessed_queue.get()\n",
    "            frame_count += 1\n",
    "\n",
    "            # Perform inference\n",
    "            interpreter.set_tensor(input_details[0]['index'], preprocessed_frame)\n",
    "            interpreter.invoke()\n",
    "            output = interpreter.get_tensor(output_details[0]['index'])\n",
    "            output = output[0]\n",
    "\n",
    "            # Apply NMS to filter out redundant bounding boxes\n",
    "            selected_indices = nms(output, threshold=0.3)  # Adjust threshold as needed\n",
    "\n",
    "            # Draw bounding boxes for selected detections after NMS\n",
    "            for index in selected_indices:\n",
    "                detection = output[:, index]\n",
    "                x_center, y_center, width, height = detection[:4]\n",
    "                x1 = max(0, int((x_center - width / 2) * orig_w))\n",
    "                y1 = max(0, int((y_center - height / 2) * orig_h))\n",
    "                x2 = min(orig_w - 1, int((x_center + width / 2) * orig_w))\n",
    "                y2 = min(orig_h - 1, int((y_center + height / 2) * orig_h))\n",
    "\n",
    "                score = np.max(detection[4:])\n",
    "                cls = np.argmax(detection[4:])\n",
    "                if score >= threshold:\n",
    "                    class_name = class_labels[str(cls)]  # Obtain the class name using the class index\n",
    "                    # Use lock to ensure safe access to frame\n",
    "                    with lock:\n",
    "                        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), thickness=2)  # Draw bounding box\n",
    "                        cv2.putText(frame, f\"{class_name}: {score:.2f}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
    "                                    (0, 255, 0), 2)\n",
    "\n",
    "            # Display the processed frame\n",
    "            with lock:\n",
    "                cv2.imshow('Object Detection', frame)\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    break\n",
    "\n",
    "    # Calculate FPS\n",
    "    end_time = cv2.getTickCount()\n",
    "    total_time = (end_time - start_time) / cv2.getTickFrequency()  # in seconds\n",
    "    fps = frame_count / total_time\n",
    "    print(f\"Average FPS: {fps}\")\n",
    "\n",
    "# Function to open a file dialog and select a video\n",
    "def open_video():\n",
    "    file_path = filedialog.askopenfilename()\n",
    "    if file_path:\n",
    "        cap = cv2.VideoCapture(file_path)\n",
    "        # Start a new thread for preprocessing\n",
    "        threading.Thread(target=preprocessing_thread, args=(cap,), daemon=True).start()\n",
    "        # Start the inference thread\n",
    "        threading.Thread(target=inference_thread, daemon=True).start()\n",
    "\n",
    "# Button to select a video\n",
    "select_button = tk.Button(root, text=\"Select Video\", command=open_video)\n",
    "select_button.pack()\n",
    "\n",
    "# Run the tkinter event loop\n",
    "root.mainloop()\n",
    "\n",
    "# Define the Non-Maximum Suppression (NMS) function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example output array\n",
    "output = np.array([\n",
    "    [10, 20, 100, 150, 0.9, 0.1, 0.05],  # Object 1\n",
    "    [30, 50, 200, 250, 0.8, 0.2, 0.1],   # Object 2\n",
    "    [80, 100, 180, 220, 0.7, 0.3, 0.6],  # Object 3\n",
    "    # ... more objects\n",
    "])\n",
    "\n",
    "# Extracting bounding box coordinates and confidence scores\n",
    "bounding_boxes = output[:, :4]\n",
    "confidence_scores = output[:, 4:]\n",
    "\n",
    "# Applying Non-Maximum Suppression (NMS)\n",
    "def nms(boxes, scores, threshold):\n",
    "    # Implementation of NMS\n",
    "    # Your implementation of NMS goes here\n",
    "    pass\n",
    "\n",
    "# Set NMS threshold\n",
    "nms_threshold = 0.5\n",
    "\n",
    "# Apply NMS\n",
    "selected_indices = nms(bounding_boxes, np.max(confidence_scores, axis=1), nms_threshold)\n",
    "\n",
    "# Display selected detections after NMS\n",
    "for index in selected_indices:\n",
    "    x_min, y_min, x_max, y_max = bounding_boxes[index]\n",
    "    max_confidence_score = np.max(confidence_scores[index])\n",
    "\n",
    "    print(\"Bounding Box Coordinates:\", (x_min, y_min, x_max, y_max))\n",
    "    print(\"Confidence Score:\", max_confidence_score)\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-18 (inference_thread):\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python311\\Lib\\threading.py\", line 1038, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\object-detection-coral\\venv-inference\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 761, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"C:\\Python311\\Lib\\threading.py\", line 975, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\Ala Arboun\\AppData\\Local\\Temp\\ipykernel_18516\\4171974782.py\", line 109, in inference_thread\n",
      "  File \"C:\\Users\\Ala Arboun\\AppData\\Local\\Temp\\ipykernel_18516\\4171974782.py\", line 72, in non_max_suppression\n",
      "IndexError: too many indices for array: array is 2-dimensional, but 3 were indexed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape before reshaping: (84, 1029)\n",
      "Output shape after reshaping: (84, 1029)\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tkinter import filedialog\n",
    "import json\n",
    "import os\n",
    "from tflite_runtime.interpreter import Interpreter, load_delegate\n",
    "import threading\n",
    "from queue import Queue\n",
    "\n",
    "# Load the TensorFlow Lite model with Edge TPU delegate\n",
    "model_path = 'yolov8m_saved_model/yolov8m_integer_quant.tflite'\n",
    "interpreter = Interpreter(model_path=model_path, experimental_delegates=[load_delegate('edgetpu.dll')])\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "input_shape = input_details[0]['shape'][1:]\n",
    "#print(\"Expected Input Dimensions:\", input_details[0]['shape'][1:])\n",
    "\n",
    "# Threshold Setting\n",
    "threshold = 0.3\n",
    "\n",
    "# Load the class labels from the JSON file\n",
    "with open('C:/object-detection-coral/label_files/labels_coco.json', 'r') as json_file:\n",
    "    class_labels = json.load(json_file)\n",
    "\n",
    "# Create a tkinter window\n",
    "root = tk.Tk()\n",
    "root.title(\"Object Detection\")\n",
    "\n",
    "# save_image_with_boxes\n",
    "output_directory = \"output_images\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Queue for preprocessed frames\n",
    "preprocessed_queue = Queue(maxsize=5)  # Adjust the size according to your needs\n",
    "\n",
    "# Lock for synchronization\n",
    "lock = threading.Lock()\n",
    "\n",
    "# Function to preprocess the frame\n",
    "def preprocess_frame(frame, input_shape):\n",
    "    # Resize frame\n",
    "    image_resized = cv2.resize(frame, (input_shape[0], input_shape[1]))\n",
    "    # Convert image to float32\n",
    "    image_float32 = image_resized.astype(np.float32)\n",
    "    # Normalize image\n",
    "    image_normalized = image_float32 / 255.0\n",
    "    # Add batch dimension\n",
    "    image_with_batch = np.expand_dims(image_normalized, 0)\n",
    "    return image_with_batch\n",
    "\n",
    "# Preprocessing thread function\n",
    "def preprocessing_thread(cap):\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # Preprocess the frame\n",
    "        preprocessed_frame = preprocess_frame(frame, input_shape)\n",
    "        # Get original frame dimensions\n",
    "        orig_h, orig_w = frame.shape[:2]\n",
    "        # Put the preprocessed frame and its dimensions into the queue\n",
    "        preprocessed_queue.put((preprocessed_frame, frame, orig_h, orig_w))\n",
    "    cap.release()\n",
    "\n",
    "# Import the necessary libraries\n",
    "import numpy as np\n",
    "\n",
    "# Function for non-maximum suppression\n",
    "def non_max_suppression(data):\n",
    "    scores = np.max(data[:, :, 4:], axis=1).reshape(-1) # Extract scores\n",
    "    areas = (data[:, :, 2] - data[:, :, 0]) * (data[:, :, 3] - data[:, :, 1]) # Calculate areas\n",
    "    index_array = scores.argsort()[::-1] # Sort indices by scores in descending order\n",
    "    keep = []\n",
    "    while index_array.size > 0:\n",
    "        keep.append(index_array[0]) # Add the current index to the keep list\n",
    "        intersection = np.maximum(0, np.prod(np.minimum(data[:, index_array[0], 2:4], data[:, index_array, 2:4]) - np.maximum(data[:, index_array[0], 0:2], data[:, index_array, 0:2]), axis=-1))\n",
    "        iou = intersection / (areas[index_array[0]] + areas[index_array] - intersection) # Calculate IoU\n",
    "        index_array = index_array[np.where(iou <= 0.5)] # Retain elements with IoU less than the threshold\n",
    "    return data[:, keep, :] # Return filtered detections\n",
    "\n",
    "# Modify the inference_thread function to incorporate non-maximum suppression\n",
    "def inference_thread():\n",
    "    frame_count = 0\n",
    "    start_time = cv2.getTickCount() # Get initial time\n",
    "    while True:\n",
    "        if not preprocessed_queue.empty():\n",
    "            # Get a preprocessed frame from the queue\n",
    "            preprocessed_frame, frame, orig_h, orig_w = preprocessed_queue.get()\n",
    "            frame_count += 1\n",
    "\n",
    "            # Perform inference\n",
    "            interpreter.set_tensor(input_details[0]['index'], preprocessed_frame)\n",
    "            interpreter.invoke()\n",
    "            output = interpreter.get_tensor(output_details[0]['index'])\n",
    "            output = output[0]\n",
    "            output = interpreter.get_tensor(output_details[0]['index'])\n",
    "            output = output[0]  # Assuming output is the first and only output tensor\n",
    "            print(\"Output shape before reshaping:\", output.shape)\n",
    "\n",
    "            # Reshape the output array for a single class\n",
    "            output = output.reshape(-1, output.shape[-1])\n",
    "\n",
    "            print(\"Output shape after reshaping:\", output.shape)\n",
    "\n",
    "            \n",
    "            # Apply non-maximum suppression\n",
    "            filtered_detections = non_max_suppression(output)\n",
    "\n",
    "            # Draw bounding boxes\n",
    "            for i in range(filtered_detections.shape[1]):\n",
    "                detection = filtered_detections[:, i]\n",
    "                x_center, y_center, width, height = detection[:4]\n",
    "                x1 = max(0, int((x_center - width / 2) * orig_w))\n",
    "                y1 = max(0, int((y_center - height / 2) * orig_h))\n",
    "                x2 = min(orig_w - 1, int((x_center + width / 2) * orig_w))\n",
    "                y2 = min(orig_h - 1, int((y_center + height / 2) * orig_h))\n",
    "\n",
    "                score = np.max(detection[4:])\n",
    "                cls = np.argmax(detection[4:])\n",
    "                if score >= threshold:\n",
    "                    class_name = class_labels[str(cls)] # Obtain the class name using the class index\n",
    "                    # Use lock to ensure safe access to frame\n",
    "                    with lock:\n",
    "                        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), thickness=2) # Draw bounding box\n",
    "                        cv2.putText(frame, f\"{class_name}: {score:.2f}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
    "                                    (0, 255, 0), 2)\n",
    "\n",
    "            # Display the processed frame\n",
    "            with lock:\n",
    "                cv2.imshow('Object Detection', frame)\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    break\n",
    "\n",
    "    # Calculate FPS\n",
    "    end_time = cv2.getTickCount()\n",
    "    total_time = (end_time - start_time) / cv2.getTickFrequency() # in seconds\n",
    "    fps = frame_count / total_time\n",
    "    print(f\"Average FPS: {fps}\")\n",
    "\n",
    "\n",
    "# Function to open a file dialog and select a video\n",
    "def open_video():\n",
    "    file_path = filedialog.askopenfilename()\n",
    "    if file_path:\n",
    "        cap = cv2.VideoCapture(file_path)\n",
    "        # Start a new thread for preprocessing\n",
    "        threading.Thread(target=preprocessing_thread, args=(cap,), daemon=True).start()\n",
    "        # Start the inference thread\n",
    "        threading.Thread(target=inference_thread, daemon=True).start()\n",
    "\n",
    "# Button to select a video\n",
    "select_button = tk.Button(root, text=\"Select Video\", command=open_video)\n",
    "select_button.pack()\n",
    "\n",
    "# Run the tkinter event loop\n",
    "root.mainloop()\n",
    "\n",
    "#with nms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output details: [{'name': 'PartitionedCall:0', 'index': 521, 'shape': array([   1,   84, 1029]), 'shape_signature': array([   1,   84, 1029]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "Sample output data: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-30 (inference_thread):\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python311\\Lib\\threading.py\", line 1038, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\object-detection-coral\\venv-inference\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 761, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"C:\\Python311\\Lib\\threading.py\", line 975, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\Ala Arboun\\AppData\\Local\\Temp\\ipykernel_18516\\2137466121.py\", line 135, in inference_thread\n",
      "ValueError: cannot reshape array of size 86436 into shape (1033)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape before reshaping: (84, 1029)\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tkinter import filedialog\n",
    "import json\n",
    "import os\n",
    "from tflite_runtime.interpreter import Interpreter, load_delegate\n",
    "import threading\n",
    "from queue import Queue\n",
    "\n",
    "# Load the TensorFlow Lite model with Edge TPU delegate\n",
    "model_path = 'yolov8m_saved_model/yolov8m_integer_quant.tflite'\n",
    "interpreter = Interpreter(model_path=model_path, experimental_delegates=[load_delegate('edgetpu.dll')])\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "print(\"Output details:\", output_details)\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "print(\"Sample output data:\", output_data[0])  # Print sample output data\n",
    "\n",
    "input_shape = input_details[0]['shape'][1:]\n",
    "#print(\"Expected Input Dimensions:\", input_details[0]['shape'][1:])\n",
    "\n",
    "# Threshold Setting\n",
    "threshold = 0.3\n",
    "\n",
    "# Load the class labels from the JSON file\n",
    "with open('C:/object-detection-coral/label_files/labels_coco.json', 'r') as json_file:\n",
    "    class_labels = json.load(json_file)\n",
    "\n",
    "# Create a tkinter window\n",
    "root = tk.Tk()\n",
    "root.title(\"Object Detection\")\n",
    "\n",
    "# save_image_with_boxes\n",
    "output_directory = \"output_images\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Queue for preprocessed frames\n",
    "preprocessed_queue = Queue(maxsize=5)  # Adjust the size according to your needs\n",
    "\n",
    "# Lock for synchronization\n",
    "lock = threading.Lock()\n",
    "\n",
    "# Function to preprocess the frame\n",
    "def preprocess_frame(frame, input_shape):\n",
    "    # Resize frame\n",
    "    image_resized = cv2.resize(frame, (input_shape[0], input_shape[1]))\n",
    "    # Convert image to float32\n",
    "    image_float32 = image_resized.astype(np.float32)\n",
    "    # Normalize image\n",
    "    image_normalized = image_float32 / 255.0\n",
    "    # Add batch dimension\n",
    "    image_with_batch = np.expand_dims(image_normalized, 0)\n",
    "    return image_with_batch\n",
    "\n",
    "# Preprocessing thread function\n",
    "def preprocessing_thread(cap):\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # Preprocess the frame\n",
    "        preprocessed_frame = preprocess_frame(frame, input_shape)\n",
    "        # Get original frame dimensions\n",
    "        orig_h, orig_w = frame.shape[:2]\n",
    "        # Put the preprocessed frame and its dimensions into the queue\n",
    "        preprocessed_queue.put((preprocessed_frame, frame, orig_h, orig_w))\n",
    "    cap.release()\n",
    "\n",
    "# Import the necessary libraries\n",
    "import numpy as np\n",
    "\n",
    "# Function for non-maximum suppression\n",
    "def non_max_suppression(data):\n",
    "    \"\"\"\n",
    "    Implements non-max suppression (NMS) for object detection.\n",
    "\n",
    "    Args:\n",
    "        data (np.ndarray): Input array of detections with shape (num_boxes, 5+num_classes)\n",
    "            - where num_boxes is the number of detected bounding boxes,\n",
    "            - 5 represents the standard bounding box coordinates (x_center, y_center, width, height, confidence)\n",
    "            - num_classes represents the number of classes predicted for each box.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of detections after applying NMS, with the same structure as the input.\n",
    "    \"\"\"\n",
    "\n",
    "    scores = data[:, 4:]  # Access scores directly (no classes)\n",
    "\n",
    "    # Calculate areas of bounding boxes\n",
    "    areas = (data[:, 2] - data[:, 0]) * (data[:, 3] - data[:, 1])\n",
    "\n",
    "    # Sort indices by scores in descending order\n",
    "    index_array = scores.argsort()[::-1]\n",
    "\n",
    "    # List to store remaining detections after NMS\n",
    "    keep = []\n",
    "\n",
    "    while index_array.size > 0:\n",
    "        # Add the current index with the highest score to the keep list\n",
    "        keep.append(index_array[0])\n",
    "\n",
    "        # Calculate Intersection-over-Union (IoU) between the current box and remaining boxes\n",
    "        intersection = np.maximum(0, np.prod(np.minimum(data[:, index_array[0], 2:4], data[:, index_array, 2:4]) - np.maximum(data[:, index_array[0], 0:2], data[:, index_array, 0:2]), axis=-1))\n",
    "        iou = intersection / (areas[index_array[0]] + areas[index_array] - intersection)\n",
    "\n",
    "        # Remove elements with IoU greater than the threshold (overlapping too much)\n",
    "        index_array = index_array[np.where(iou <= 0.5)]  # Retain elements with IoU less than the threshold\n",
    "\n",
    "    # Return only the detections that were kept after NMS\n",
    "    return data[:, keep, :]\n",
    "\n",
    "\n",
    "# Modify the inference_thread function to incorporate non-maximum suppression\n",
    "def inference_thread():\n",
    "    frame_count = 0\n",
    "    start_time = cv2.getTickCount()  # Get initial time\n",
    "    while True:\n",
    "        if not preprocessed_queue.empty():\n",
    "            # Get a preprocessed frame from the queue\n",
    "            preprocessed_frame, frame, orig_h, orig_w = preprocessed_queue.get()\n",
    "            frame_count += 1\n",
    "\n",
    "            # Perform inference\n",
    "            interpreter.set_tensor(input_details[0]['index'], preprocessed_frame)\n",
    "            interpreter.invoke()\n",
    "            output = interpreter.get_tensor(output_details[0]['index'])\n",
    "            output = output[0]  # Assuming output is the first and only output tensor\n",
    "            print(\"Output shape before reshaping:\", output.shape)\n",
    "\n",
    "            # Reshape the output array for a single class\n",
    "           \n",
    "            output = output.reshape(-1, 4 + output.shape[-1])  # Reshape for single class, including scores\n",
    "\n",
    "\n",
    "            print(\"Output shape after reshaping:\", output.shape)\n",
    "\n",
    "            # Apply non-maximum suppression\n",
    "            filtered_detections = non_max_suppression(output)\n",
    "\n",
    "            # Draw bounding boxes\n",
    "            for i in range(filtered_detections.shape[1]):\n",
    "                detection = filtered_detections[:, i]\n",
    "                x_center, y_center, width, height = detection[:4]\n",
    "                x1 = max(0, int((x_center - width / 2) * orig_w))\n",
    "                y1 = max(0, int((y_center - height / 2) * orig_h))\n",
    "                x2 = min(orig_w - 1, int((x_center + width / 2) * orig_w))\n",
    "                y2 = min(orig_h - 1, int((y_center + height / 2) * orig_h))\n",
    "\n",
    "                score = np.max(detection[4:])\n",
    "                cls = np.argmax(detection[4:])\n",
    "                if score >= threshold:\n",
    "                    class_name = class_labels[str(cls)]  # Obtain the class name using the class index\n",
    "                    # Use lock to ensure safe access to frame\n",
    "                    with lock:\n",
    "                        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), thickness=2)  # Draw bounding box\n",
    "                        cv2.putText(frame, f\"{class_name}: {score:.2f}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
    "                                    (0, 255, 0), 2)\n",
    "\n",
    "            # Display the processed frame\n",
    "            with lock:\n",
    "                cv2.imshow('Object Detection', frame)\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    break\n",
    "\n",
    "    # Calculate FPS\n",
    "    end_time = cv2.getTickCount()\n",
    "    total_time = (end_time - start_time) / cv2.getTickFrequency()  # in seconds\n",
    "    fps = frame_count / total_time\n",
    "    print(f\"Average FPS: {fps}\")\n",
    "\n",
    "\n",
    "# Function to open a file dialog and select a video\n",
    "def open_video():\n",
    "    file_path = filedialog.askopenfilename()\n",
    "    if file_path:\n",
    "        cap = cv2.VideoCapture(file_path)\n",
    "        # Start a new thread for preprocessing\n",
    "        threading.Thread(target=preprocessing_thread, args=(cap,), daemon=True).start()\n",
    "        # Start the inference thread\n",
    "        threading.Thread(target=inference_thread, daemon=True).start()\n",
    "\n",
    "# Button to select a video\n",
    "select_button = tk.Button(root, text=\"Select Video\", command=open_video)\n",
    "select_button.pack()\n",
    "\n",
    "# Run the tkinter event loop\n",
    "root.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-inference",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
