{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tkinter import filedialog\n",
    "import json\n",
    "import os\n",
    "from tflite_runtime.interpreter import Interpreter, load_delegate\n",
    "import threading\n",
    "from queue import Queue\n",
    "import time\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import tkinter as tk\n",
    "\n",
    "class ObjectDetector:\n",
    "    def __init__(self, model_path, label_path, threshold=0.3):\n",
    "        self.threshold = threshold\n",
    "        self.interpreter = Interpreter(model_path=model_path, experimental_delegates=[load_delegate('edgetpu.dll')])\n",
    "        self.interpreter.allocate_tensors()\n",
    "        self.input_details = self.interpreter.get_input_details()\n",
    "        self.output_details = self.interpreter.get_output_details()\n",
    "        self.input_shape = self.input_details[0]['shape'][1:]\n",
    "        self.class_labels = self.load_class_labels(label_path)\n",
    "\n",
    "        self.frame_count = 0\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def load_class_labels(self, label_path):\n",
    "        with open(label_path, 'r') as json_file:\n",
    "            class_labels = json.load(json_file)\n",
    "        return class_labels\n",
    "\n",
    "    def preprocess_frame(self, frame):\n",
    "        image_resized = cv2.resize(frame, (self.input_shape[0], self.input_shape[1]))\n",
    "        image_float32 = image_resized.astype(np.float32)\n",
    "        image_normalized = image_float32 / 255.0\n",
    "        image_with_batch = np.expand_dims(image_normalized, 0)\n",
    "        return image_with_batch\n",
    "\n",
    "    def nms(self, boxes, scores, threshold):\n",
    "        if len(boxes) == 0:\n",
    "            return []\n",
    "        x1 = boxes[:, 0]\n",
    "        y1 = boxes[:, 1]\n",
    "        x2 = boxes[:, 2]\n",
    "        y2 = boxes[:, 3]\n",
    "        areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "        order = scores.argsort()[::-1]\n",
    "        keep = []\n",
    "        while order.size > 0:\n",
    "            i = order[0]\n",
    "            keep.append(i)\n",
    "            xx1 = np.maximum(x1[i], x1[order[1:]])\n",
    "            yy1 = np.maximum(y1[i], y1[order[1:]])\n",
    "            xx2 = np.minimum(x2[i], x2[order[1:]])\n",
    "            yy2 = np.minimum(y2[i], y2[order[1:]])\n",
    "            w = np.maximum(0.0, xx2 - xx1 + 1)\n",
    "            h = np.maximum(0.0, yy2 - yy1 + 1)\n",
    "            overlap = (w * h) / areas[order[1:]]\n",
    "            inds = np.where(overlap <= threshold)[0]\n",
    "            order = order[inds + 1]\n",
    "        return keep\n",
    "    \n",
    "    def perform_inference(self, frame, preprocessed_frame, orig_h, orig_w):\n",
    "        self.interpreter.set_tensor(self.input_details[0]['index'], preprocessed_frame)\n",
    "        self.interpreter.invoke()\n",
    "        output_data = self.interpreter.get_tensor(self.output_details[0]['index'])\n",
    "        output = np.copy(output_data[0])\n",
    "\n",
    "        boxes = []\n",
    "        scores = []\n",
    "\n",
    "        for i in range(output.shape[1]):\n",
    "            detection = output[:, i]\n",
    "            x_center, y_center, width, height = detection[:4]\n",
    "            x1 = max(0, int((x_center - width / 2) * orig_w))\n",
    "            y1 = max(0, int((y_center - height / 2) * orig_h))\n",
    "            x2 = min(orig_w - 1, int((x_center + width / 2) * orig_w))\n",
    "            y2 = min(orig_h - 1, int((y_center + height / 2) * orig_h))\n",
    "            score = np.max(detection[4:])\n",
    "            boxes.append([x1, y1, x2, y2])\n",
    "            scores.append(score)\n",
    "\n",
    "        boxes = np.array(boxes)\n",
    "        scores = np.array(scores)\n",
    "        keep = self.nms(boxes, scores, 0.7)\n",
    "\n",
    "        for idx in keep:\n",
    "            x1, y1, x2, y2 = boxes[idx]\n",
    "            score = scores[idx]\n",
    "            detection = output[:, idx]\n",
    "            cls = np.argmax(output[:, idx][4:])\n",
    "            if score >= self.threshold:\n",
    "                class_name = self.class_labels[str(cls)]\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), thickness=2)\n",
    "                cv2.putText(frame, f\"{class_name}: {score:.2f}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "        self.frame_count += 1\n",
    "\n",
    "    def get_fps(self):\n",
    "        current_time = time.time()\n",
    "        elapsed_time = current_time - self.start_time\n",
    "        if elapsed_time > 0:\n",
    "            fps = self.frame_count / elapsed_time\n",
    "            return fps\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def detect_objects(self, frame):\n",
    "        preprocessed_frame = self.preprocess_frame(frame)\n",
    "        orig_h, orig_w = frame.shape[:2]\n",
    "        self.perform_inference(frame, preprocessed_frame, orig_h, orig_w)\n",
    "\n",
    "    def run_detection(self, cap, output_queue):\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            output_queue.put(frame)\n",
    "\n",
    "        cap.release()\n",
    "        output_queue.put(None)\n",
    "\n",
    "class App:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"Object Detection\")\n",
    "        self.detector = None\n",
    "\n",
    "        select_button = tk.Button(root, text=\"Select Video\", command=self.open_video)\n",
    "        select_button.pack()\n",
    "\n",
    "    def open_video(self):\n",
    "        file_path = filedialog.askopenfilename()\n",
    "        if file_path:\n",
    "            cap = cv2.VideoCapture(file_path)\n",
    "            output_queue = Queue()\n",
    "            self.detector = ObjectDetector(\n",
    "                model_path='yolov8m_saved_model\\yolov8m_integer_quant.tflite',\n",
    "                label_path='C:/object-detection-coral/label_files/labels_coco.json'\n",
    "            )\n",
    "            detection_thread = threading.Thread(target=self.detector.run_detection, args=(cap, output_queue))\n",
    "            detection_thread.start()\n",
    "            self.process_output(output_queue)\n",
    "\n",
    "    def process_output(self, output_queue):\n",
    "        while True:\n",
    "            frame = output_queue.get()\n",
    "            if frame is None:\n",
    "                break\n",
    "            self.detector.detect_objects(frame)\n",
    "            cv2.putText(frame, f\"FPS: {self.detector.get_fps():.2f}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            cv2.imshow('Object Detection', frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    app = App(root)\n",
    "    root.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-inference",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
